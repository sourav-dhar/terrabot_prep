{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrapay Transaction Monitoring Analysis\n",
    "\n",
    "This notebook conducts a thorough analysis of Terrapay's transaction monitoring system to optimize rule efficiency and reduce false positives.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Identify KYC IDs that have alerted in the last 3 months across rules\n",
    "2. Determine what percentage of KYC IDs alerted on multiple rules\n",
    "3. Analyze true positive vs false positive rates for rules\n",
    "4. Examine KYC breakage impact on rule efficiency\n",
    "5. Identify redundant or overlapping rules\n",
    "6. Generate recommendations for rule optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "Let's import the necessary libraries and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import os\n",
    "from fuzzywuzzy import fuzz  # For name similarity matching\n",
    "import networkx as nx\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create output directory for visualizations\n",
    "os.makedirs('visualizations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data from Excel file\n",
    "def load_data():\n",
    "    \"\"\"Load all data from Excel file\"\"\"\n",
    "    # Read transaction data\n",
    "    transaction_data = pd.read_excel('transaction_dummy_data_10k_final.xlsx', \n",
    "                                     sheet_name='transaction_dummy_data_10k')\n",
    "    \n",
    "    # Read metadata\n",
    "    metadata = pd.read_excel('transaction_dummy_data_10k_final.xlsx', \n",
    "                             sheet_name='Sheet2')\n",
    "    \n",
    "    # Read rule descriptions\n",
    "    rule_descriptions = pd.read_excel('transaction_dummy_data_10k_final.xlsx', \n",
    "                                     sheet_name='rule_description')\n",
    "    \n",
    "    # Convert dates to datetime if not already\n",
    "    date_columns = ['transaction_date_time_local', 'created_at', 'closed_at', \n",
    "                    'kyc_sender_create_date', 'kyc_receiver_create_date',\n",
    "                    'dob_sender', 'dob_receiver', 'self_closure_date']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in transaction_data.columns:\n",
    "            transaction_data[col] = pd.to_datetime(transaction_data[col])\n",
    "    \n",
    "    return transaction_data, metadata, rule_descriptions\n",
    "\n",
    "# Load all data\n",
    "transaction_data, metadata, rule_descriptions = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploratory analysis\n",
    "print(f\"Dataset shape: {transaction_data.shape}\")\n",
    "print(f\"Data timeframe: {transaction_data['transaction_date_time_local'].min().date()} to {transaction_data['transaction_date_time_local'].max().date()}\")\n",
    "\n",
    "# Basic stats about the data\n",
    "print(\"\\nStatus distribution:\")\n",
    "status_counts = transaction_data['status'].value_counts()\n",
    "print(status_counts)\n",
    "\n",
    "# Plot status distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=status_counts.index, y=status_counts.values)\n",
    "plt.title('Alert Status Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRule frequency distribution:\")\n",
    "print(transaction_data['rule_frequency'].value_counts())\n",
    "\n",
    "print(\"\\nRule pattern distribution:\")\n",
    "print(transaction_data['rule_pattern'].value_counts())\n",
    "\n",
    "print(\"\\nTop 10 most frequent alerting rules:\")\n",
    "top_rules = transaction_data['alert_rules'].value_counts().head(10)\n",
    "print(top_rules)\n",
    "\n",
    "# Plot top rules\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_rules.index, y=top_rules.values)\n",
    "plt.title('Top 10 Most Frequent Alerting Rules')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of triggered_on (sender vs receiver)\n",
    "print(\"\\nDistribution of triggered_on:\")\n",
    "print(transaction_data['triggered_on'].value_counts())\n",
    "\n",
    "# Count of unique KYC IDs\n",
    "print(\"\\nUnique KYC IDs:\")\n",
    "print(f\"Sender KYC IDs: {transaction_data['sender_kyc_id_no'].nunique()}\")\n",
    "print(f\"Receiver KYC IDs: {transaction_data['receiver_kyc_id_no'].nunique()}\")\n",
    "\n",
    "# Display sample of rule descriptions\n",
    "print(\"\\nRule descriptions sample:\")\n",
    "print(rule_descriptions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KYC Alert Overlap Analysis\n",
    "\n",
    "In this section, we'll identify KYC IDs that have alerted across multiple rules to understand the overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kyc_alert_overlap(transaction_data):\n",
    "    \"\"\"Analyze KYC IDs that have alerted across rules and determine overlap.\"\"\"\n",
    "    print(\"Analyzing KYC IDs that have alerted...\")\n",
    "    \n",
    "    # Group alerts by KYC ID (based on triggered_on field)\n",
    "    kyc_alerts = defaultdict(set)\n",
    "    \n",
    "    for idx, row in transaction_data.iterrows():\n",
    "        if row['triggered_on'] == 'sender':\n",
    "            kyc_id = row['sender_kyc_id_no']\n",
    "        else:  # receiver\n",
    "            kyc_id = row['receiver_kyc_id_no']\n",
    "            \n",
    "        kyc_alerts[kyc_id].add(row['alert_rules'])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_kyc_with_alerts = len(kyc_alerts)\n",
    "    kyc_with_multiple_rules = sum(1 for rules in kyc_alerts.values() if len(rules) > 1)\n",
    "    \n",
    "    # Distribution of number of rules per KYC\n",
    "    rule_count_per_kyc = [len(rules) for rules in kyc_alerts.values()]\n",
    "    rule_count_distribution = pd.Series(rule_count_per_kyc).value_counts().sort_index()\n",
    "    \n",
    "    # Calculate overlap percentage\n",
    "    overlap_percentage = (kyc_with_multiple_rules / total_kyc_with_alerts) * 100 if total_kyc_with_alerts > 0 else 0\n",
    "    \n",
    "    print(f\"Total KYC IDs with alerts: {total_kyc_with_alerts}\")\n",
    "    print(f\"KYC IDs alerting on multiple rules: {kyc_with_multiple_rules}\")\n",
    "    print(f\"Percentage of KYC IDs alerting on multiple rules: {overlap_percentage:.2f}%\")\n",
    "    \n",
    "    print(\"\\nDistribution of number of rules per KYC ID:\")\n",
    "    print(rule_count_distribution)\n",
    "    \n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    rule_count_distribution.plot(kind='bar')\n",
    "    plt.title('Number of Rules Triggered per KYC ID')\n",
    "    plt.xlabel('Number of Rules')\n",
    "    plt.ylabel('Count of KYC IDs')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find co-occurring rules\n",
    "    rule_pairs = []\n",
    "    for rules in kyc_alerts.values():\n",
    "        if len(rules) > 1:\n",
    "            # Convert set to array for easier processing\n",
    "            rule_list = list(rules)\n",
    "            for i in range(len(rule_list)):\n",
    "                for j in range(i+1, len(rule_list)):\n",
    "                    rule_pairs.append((rule_list[i], rule_list[j]))\n",
    "    \n",
    "    # Count occurrences of each rule pair\n",
    "    rule_pair_counts = pd.Series(rule_pairs).value_counts().head(15)\n",
    "    \n",
    "    print(\"\\nTop 15 co-occurring rule pairs:\")\n",
    "    print(rule_pair_counts)\n",
    "    \n",
    "    # Create and plot co-occurrence matrix if we have rule pairs\n",
    "    if rule_pairs:\n",
    "        unique_rules = sorted(set(rule for pair in rule_pairs for rule in pair))\n",
    "        \n",
    "        # Only create a heatmap if not too large\n",
    "        if len(unique_rules) <= 30:  \n",
    "            cooccurrence_matrix = pd.DataFrame(0, index=unique_rules, columns=unique_rules)\n",
    "            \n",
    "            for r1, r2 in rule_pairs:\n",
    "                cooccurrence_matrix.loc[r1, r2] += 1\n",
    "                cooccurrence_matrix.loc[r2, r1] += 1\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(cooccurrence_matrix, cmap=\"YlGnBu\", annot=False)\n",
    "            plt.title('Rule Co-occurrence Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return kyc_alerts, rule_count_distribution, rule_pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the KYC alert overlap analysis\n",
    "kyc_alerts, rule_count_dist, rule_pairs = analyze_kyc_alert_overlap(transaction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rule Efficiency Analysis (True Positives vs False Positives)\n",
    "\n",
    "Next, we'll analyze the efficiency of each rule based on true positive and false positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rule_efficiency(transaction_data, rule_descriptions):\n",
    "    \"\"\"Analyze the efficiency of rules based on true positive and false positive rates.\"\"\"\n",
    "    print(\"Analyzing rule efficiency based on true/false positives...\")\n",
    "    \n",
    "    # Filter for closed alerts only (where investigation is complete)\n",
    "    closed_alerts = transaction_data[transaction_data['status'].isin(['Closed TP', 'Closed FP'])]\n",
    "    \n",
    "    # Overall TP/FP rates\n",
    "    true_positive_rate = len(closed_alerts[closed_alerts['status'] == 'Closed TP']) / len(closed_alerts) * 100\n",
    "    false_positive_rate = len(closed_alerts[closed_alerts['status'] == 'Closed FP']) / len(closed_alerts) * 100\n",
    "    \n",
    "    print(f\"Overall True Positive Rate: {true_positive_rate:.2f}%\")\n",
    "    print(f\"Overall False Positive Rate: {false_positive_rate:.2f}%\")\n",
    "    \n",
    "    # Create a performance dataframe for each rule\n",
    "    rule_performance = closed_alerts.groupby('alert_rules').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Total': len(x),\n",
    "            'TP': sum(x['status'] == 'Closed TP'),\n",
    "            'FP': sum(x['status'] == 'Closed FP'),\n",
    "            'TP_Rate': sum(x['status'] == 'Closed TP') / len(x) * 100 if len(x) > 0 else 0,\n",
    "            'Frequency': x['rule_frequency'].iloc[0] if not x['rule_frequency'].empty else 'Unknown',\n",
    "            'Pattern': x['rule_pattern'].iloc[0] if not x['rule_pattern'].empty else 'Unknown'\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge with rule descriptions\n",
    "    rule_performance = rule_performance.merge(\n",
    "        rule_descriptions[['Rule no.', 'Rule description', 'Current threshold']], \n",
    "        left_on='alert_rules', \n",
    "        right_on='Rule no.', \n",
    "        how='left'\n",
    "    ).drop('Rule no.', axis=1)\n",
    "    \n",
    "    # Sort by TP rate descending\n",
    "    rule_performance_by_tp = rule_performance.sort_values('TP_Rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nRule performance by true positive rate (Top 10):\")\n",
    "    print(rule_performance_by_tp[['alert_rules', 'Total', 'TP', 'FP', 'TP_Rate', 'Frequency', 'Pattern']].head(10))\n",
    "    \n",
    "    print(\"\\nRule performance by true positive rate (Bottom 10):\")\n",
    "    print(rule_performance_by_tp[['alert_rules', 'Total', 'TP', 'FP', 'TP_Rate', 'Frequency', 'Pattern']].tail(10))\n",
    "    \n",
    "    # Find inefficient rules (high volume, low TP rate)\n",
    "    inefficient_rules = rule_performance[(rule_performance['Total'] > 50) & \n",
    "                                         (rule_performance['TP_Rate'] < 30)].sort_values('Total', ascending=False)\n",
    "    \n",
    "    print(\"\\nInefficient rules (high volume, low TP rate):\")\n",
    "    print(inefficient_rules[['alert_rules', 'Total', 'TP', 'FP', 'TP_Rate', 'Frequency', 'Pattern']].head(10))\n",
    "    \n",
    "    # Analyze performance by pattern\n",
    "    pattern_performance = rule_performance.groupby('Pattern').agg({\n",
    "        'Total': 'sum',\n",
    "        'TP': 'sum',\n",
    "        'FP': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    pattern_performance['TP_Rate'] = pattern_performance['TP'] / pattern_performance['Total'] * 100\n",
    "    pattern_performance = pattern_performance.sort_values('TP_Rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nPerformance by rule pattern:\")\n",
    "    print(pattern_performance)\n",
    "    \n",
    "    # Analyze performance by frequency\n",
    "    frequency_performance = rule_performance.groupby('Frequency').agg({\n",
    "        'Total': 'sum',\n",
    "        'TP': 'sum',\n",
    "        'FP': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    frequency_performance['TP_Rate'] = frequency_performance['TP'] / frequency_performance['Total'] * 100\n",
    "    frequency_performance = frequency_performance.sort_values('TP_Rate', ascending=False)\n",
    "    \n",
    "    print(\"\\nPerformance by rule frequency:\")\n",
    "    print(frequency_performance)\n",
    "    \n",
    "    # Plot TP rate by rule (top 20 by volume)\n",
    "    top_rules_by_volume = rule_performance.sort_values('Total', ascending=False).head(20)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    bars = plt.bar(top_rules_by_volume['alert_rules'], top_rules_by_volume['TP_Rate'], \n",
    "            color=plt.cm.RdYlGn(top_rules_by_volume['TP_Rate']/100), alpha=0.7)\n",
    "    plt.axhline(y=50, color='r', linestyle='--', label='50% TP Rate')\n",
    "    plt.title('True Positive Rate for Top 20 Rules by Volume')\n",
    "    plt.xlabel('Rule')\n",
    "    plt.ylabel('True Positive Rate (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot by pattern\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Pattern', y='TP_Rate', data=pattern_performance, palette='viridis')\n",
    "    plt.title('True Positive Rate by Rule Pattern')\n",
    "    plt.xlabel('Pattern')\n",
    "    plt.ylabel('True Positive Rate (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot by frequency\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Frequency', y='TP_Rate', data=frequency_performance, palette='plasma')\n",
    "    plt.title('True Positive Rate by Rule Frequency')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('True Positive Rate (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get list of true positives\n",
    "    true_positives = transaction_data[transaction_data['status'] == 'Closed TP']\n",
    "    \n",
    "    # Extract unique KYC IDs with true positive alerts\n",
    "    tp_kyc_ids = []\n",
    "    for idx, row in true_positives.iterrows():\n",
    "        if row['triggered_on'] == 'sender':\n",
    "            tp_kyc_ids.append(row['sender_kyc_id_no'])\n",
    "        else:  # receiver\n",
    "            tp_kyc_ids.append(row['receiver_kyc_id_no'])\n",
    "    \n",
    "    unique_tp_kyc_ids = set(tp_kyc_ids)\n",
    "    \n",
    "    print(f\"\\nTotal true positive alerts: {len(true_positives)}\")\n",
    "    print(f\"Number of unique KYC IDs with true positive alerts: {len(unique_tp_kyc_ids)}\")\n",
    "    \n",
    "    return rule_performance, pattern_performance, frequency_performance, true_positives, unique_tp_kyc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the rule efficiency analysis\n",
    "rule_performance, pattern_performance, frequency_performance, true_positives, tp_kyc_ids = analyze_rule_efficiency(transaction_data, rule_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KYC Breakage Analysis\n",
    "\n",
    "In this section, we'll analyze the KYC breakage issue where multiple KYC IDs exist for the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kyc_breakage(transaction_data):\n",
    "    \"\"\"Analyze the KYC breakage issue (multiple KYC IDs for the same person).\"\"\"\n",
    "    print(\"\\nAnalyzing KYC breakage issue...\")\n",
    "    \n",
    "    # Analyze sender KYC breakage\n",
    "    print(\"\\n=== Sender KYC Breakage Analysis ===\")\n",
    "    \n",
    "    # Group by sender name and count KYC IDs\n",
    "    sender_name_groups = transaction_data.groupby('sender_name_kyc_wise')['sender_kyc_id_no'].nunique().reset_index()\n",
    "    sender_name_groups.columns = ['sender_name', 'kyc_id_count']\n",
    "    \n",
    "    # Distribution of KYC IDs per sender name\n",
    "    sender_kyc_distribution = sender_name_groups['kyc_id_count'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"Distribution of KYC IDs per unique sender name:\")\n",
    "    print(sender_kyc_distribution.head(10))\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_kyc_per_sender = sender_name_groups['kyc_id_count'].mean()\n",
    "    max_kyc_per_sender = sender_name_groups['kyc_id_count'].max()\n",
    "    senders_with_multiple_kyc = sum(sender_name_groups['kyc_id_count'] > 1)\n",
    "    percentage_senders_with_multiple_kyc = senders_with_multiple_kyc / len(sender_name_groups) * 100\n",
    "    \n",
    "    print(f\"\\nAverage KYC IDs per unique sender name: {avg_kyc_per_sender:.2f}\")\n",
    "    print(f\"Maximum KYC IDs for a single sender name: {max_kyc_per_sender}\")\n",
    "    print(f\"Senders with multiple KYC IDs: {senders_with_multiple_kyc} ({percentage_senders_with_multiple_kyc:.2f}%)\")\n",
    "    \n",
    "    # Find senders with the most KYC IDs\n",
    "    top_multiple_kyc_senders = sender_name_groups.sort_values('kyc_id_count', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 senders with most KYC IDs:\")\n",
    "    print(top_multiple_kyc_senders)\n",
    "    \n",
    "    # Analyze receiver KYC breakage\n",
    "    print(\"\\n=== Receiver KYC Breakage Analysis ===\")\n",
    "    \n",
    "    # Group by receiver name and count KYC IDs\n",
    "    receiver_name_groups = transaction_data.groupby('receiver_name_kyc_wise')['receiver_kyc_id_no'].nunique().reset_index()\n",
    "    receiver_name_groups.columns = ['receiver_name', 'kyc_id_count']\n",
    "    \n",
    "    # Distribution of KYC IDs per receiver name\n",
    "    receiver_kyc_distribution = receiver_name_groups['kyc_id_count'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"Distribution of KYC IDs per unique receiver name:\")\n",
    "    print(receiver_kyc_distribution.head(10))\n",
    "    \n",
    "    # Calculate stats\n",
    "    avg_kyc_per_receiver = receiver_name_groups['kyc_id_count'].mean()\n",
    "    max_kyc_per_receiver = receiver_name_groups['kyc_id_count'].max()\n",
    "    receivers_with_multiple_kyc = sum(receiver_name_groups['kyc_id_count'] > 1)\n",
    "    percentage_receivers_with_multiple_kyc = receivers_with_multiple_kyc / len(receiver_name_groups) * 100\n",
    "    \n",
    "    print(f\"\\nAverage KYC IDs per unique receiver name: {avg_kyc_per_receiver:.2f}\")\n",
    "    print(f\"Maximum KYC IDs for a single receiver name: {max_kyc_per_receiver}\")\n",
    "    print(f\"Receivers with multiple KYC IDs: {receivers_with_multiple_kyc} ({percentage_receivers_with_multiple_kyc:.2f}%)\")\n",
    "    \n",
    "    # Plot KYC breakage distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sender_kyc_distribution.head(5).plot(kind='bar')\n",
    "    plt.title('KYC IDs per Sender Name')\n",
    "    plt.xlabel('Number of KYC IDs')\n",
    "    plt.ylabel('Count of Sender Names')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    receiver_kyc_distribution.head(5).plot(kind='bar')\n",
    "    plt.title('KYC IDs per Receiver Name')\n",
    "    plt.xlabel('Number of KYC IDs')\n",
    "    plt.ylabel('Count of Receiver Names')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze impact on receiver rules\n",
    "    print(\"\\n=== Impact on Receiver Rules ===\")\n",
    "    receiver_alerts = transaction_data[transaction_data['triggered_on'] == 'receiver']\n",
    "    \n",
    "    # Find receivers with multiple KYC IDs\n",
    "    receivers_with_multiple_kyc_ids = receiver_name_groups[receiver_name_groups['kyc_id_count'] > 1]['receiver_name'].tolist()\n",
    "    \n",
    "    # Filter alerts for these receivers\n",
    "    if receivers_with_multiple_kyc_ids:\n",
    "        multiple_kyc_receiver_alerts = receiver_alerts[receiver_alerts['receiver_name_kyc_wise'].isin(receivers_with_multiple_kyc_ids)]\n",
    "        \n",
    "        # Count alerts by rule for these receivers\n",
    "        if not multiple_kyc_receiver_alerts.empty:\n",
    "            multi_kyc_rule_counts = multiple_kyc_receiver_alerts.groupby('alert_rules').size().sort_values(ascending=False)\n",
    "            \n",
    "            print(\"\\nDistribution of rules for receivers with multiple KYC IDs:\")\n",
    "            print(multi_kyc_rule_counts.head(10))\n",
    "            \n",
    "            # Calculate percentage of alerts that might be affected by KYC breakage\n",
    "            pct_receiver_alerts_affected = len(multiple_kyc_receiver_alerts) / len(receiver_alerts) * 100\n",
    "            print(f\"\\nPercentage of receiver alerts potentially affected by KYC breakage: {pct_receiver_alerts_affected:.2f}%\")\n",
    "    \n",
    "    return sender_name_groups, receiver_name_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the KYC breakage analysis\n",
    "sender_name_groups, receiver_name_groups = analyze_kyc_breakage(transaction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rule Overlap and Recommendations\n",
    "\n",
    "Finally, we'll identify rule overlap and generate recommendations for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_rule_overlap_and_generate_recommendations(transaction_data, rule_descriptions, rule_performance, kyc_alerts):\n",
    "    \"\"\"Identify rule overlap and generate recommendations.\"\"\"\n",
    "    print(\"\\nIdentifying rule overlap and generating recommendations...\")\n",
    "    \n",
    "    # Create a correlation matrix for rules\n",
    "    all_rules = transaction_data['alert_rules'].unique()\n",
    "    cooccurrence_matrix = pd.DataFrame(0, index=all_rules, columns=all_rules)\n",
    "    \n",
    "    # Fill the co-occurrence matrix using the kyc_alerts dictionary\n",
    "    for kyc_id, rules in kyc_alerts.items():\n",
    "        rule_list = list(rules)\n",
    "        for i in range(len(rule_list)):\n",
    "            for j in range(i, len(rule_list)):\n",
    "                cooccurrence_matrix.loc[rule_list[i], rule_list[j]] += 1\n",
    "                if i != j:\n",
    "                    cooccurrence_matrix.loc[rule_list[j], rule_list[i]] += 1\n",
    "    \n",
    "    # Convert to correlation matrix (Jaccard similarity)\n",
    "    correlation_matrix = pd.DataFrame(0.0, index=all_rules, columns=all_rules)\n",
    "    \n",
    "    for i in all_rules:\n",
    "        for j in all_rules:\n",
    "            if i == j:\n",
    "                correlation_matrix.loc[i, j] = 1.0\n",
    "            else:\n",
    "                intersection = cooccurrence_matrix.loc[i, j]\n",
    "                union = cooccurrence_matrix.loc[i, i] + cooccurrence_matrix.loc[j, j] - intersection\n",
    "                correlation_matrix.loc[i, j] = intersection / union if union > 0 else 0\n",
    "    \n",
    "    # Identify highly correlated rule pairs\n",
    "    high_correlation_threshold = 0.7\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(all_rules)):\n",
    "        for j in range(i+1, len(all_rules)):\n",
    "            rule1, rule2 = all_rules[i], all_rules[j]\n",
    "            corr = correlation_matrix.loc[rule1, rule2]\n",
    "            if corr >= high_correlation_threshold:\n",
    "                high_corr_pairs.append((rule1, rule2, corr))\n",
    "    \n",
    "    # Sort by correlation\n",
    "    high_corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(f\"\\nIdentified {len(high_corr_pairs)} rule pairs with correlation >= {high_correlation_threshold}\")\n",
    "    if high_corr_pairs:\n",
    "        print(\"Top 5 correlated pairs:\")\n",
    "        for rule1, rule2, corr in high_corr_pairs[:5]:\n",
    "            print(f\"  {rule1} and {rule2}: {corr:.3f}\")\n",
    "            \n",
    "    # Visualize rule correlation as a heatmap\n",
    "    if len(all_rules) <= 30:  # Only plot if not too many rules\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix, cmap='viridis', annot=False)\n",
    "        plt.title('Rule Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Get rule information\n",
    "    rule_info = rule_descriptions.set_index('Rule no.').to_dict()\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Consolidate highly correlated rules\n",
    "    for rule1, rule2, corr in high_corr_pairs[:10]:\n",
    "        if corr > 0.8:  # Very high correlation\n",
    "            # Get rule details from rule_performance DataFrame\n",
    "            rule1_perf = rule_performance[rule_performance['alert_rules'] == rule1]\n",
    "            rule2_perf = rule_performance[rule_performance['alert_rules'] == rule2]\n",
    "            \n",
    "            if not rule1_perf.empty and not rule2_perf.empty:\n",
    "                rule1_tp = rule1_perf.iloc[0]['TP_Rate']\n",
    "                rule2_tp = rule2_perf.iloc[0]['TP_Rate']\n",
    "                \n",
    "                keep_rule = rule1 if rule1_tp >= rule2_tp else rule2\n",
    "                remove_rule = rule2 if keep_rule == rule1 else rule1\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'Category': 'Consolidate Similar Rules',\n",
    "                    'Rules': f\"{rule1}, {rule2}\",\n",
    "                    'Action': f\"Combine rules, keep {keep_rule}\",\n",
    "                    'Rationale': f\"High correlation ({corr:.2f}), {keep_rule} has higher TP rate ({max(rule1_tp, rule2_tp):.1f}%)\",\n",
    "                    'Impact': 'Reduce alert volume while maintaining effectiveness'\n",
    "                })\n",
    "    \n",
    "    # 2. Remove or modify inefficient rules\n",
    "    inefficient_rules = rule_performance[(rule_performance['Total'] > 50) & \n",
    "                                          (rule_performance['TP_Rate'] < 30)].sort_values('Total', ascending=False)\n",
    "    \n",
    "    for _, rule in inefficient_rules.head(5).iterrows():\n",
    "        recommendations.append({\n",
    "            'Category': 'Optimize Inefficient Rules',\n",
    "            'Rules': rule['alert_rules'],\n",
    "            'Action': 'Increase threshold or consider removing',\n",
    "            'Rationale': f\"Low TP rate ({rule['TP_Rate']:.1f}%) with high volume ({rule['Total']} alerts)\",\n",
    "            'Impact': 'Significant reduction in false positives'\n",
    "        })\n",
    "    \n",
    "    # 3. KYC breakage mitigation\n",
    "    recommendations.append({\n",
    "        'Category': 'KYC Breakage Mitigation',\n",
    "        'Rules': 'All receiver-focused rules',\n",
    "        'Action': 'Implement name/phone fuzzy matching before rule execution',\n",
    "        'Rationale': 'Multiple KYC IDs for the same entity causing duplicate alerts',\n",
    "        'Impact': 'More accurate entity identification, reduced false positives'\n",
    "    })\n",
    "    \n",
    "    # 4. Pattern-based recommendations\n",
    "    pattern_perf = rule_performance.groupby('Pattern')['TP_Rate'].mean().sort_values()\n",
    "    \n",
    "    for pattern, tp_rate in pattern_perf.items():\n",
    "        if tp_rate < 40:\n",
    "            pattern_rules = rule_performance[rule_performance['Pattern'] == pattern]\n",
    "            recommendations.append({\n",
    "                'Category': 'Pattern-Based Optimization',\n",
    "                'Rules': f\"{pattern} pattern rules\",\n",
    "                'Action': f\"Review and adjust thresholds for {pattern} rules\",\n",
    "                'Rationale': f\"Low average TP rate ({tp_rate:.1f}%) for {pattern} pattern\",\n",
    "                'Impact': 'Improve detection effectiveness for this pattern type'\n",
    "            })\n",
    "    \n",
    "    # 5. Frequency-based recommendations\n",
    "    daily_rules = rule_performance[(rule_performance['Frequency'] == 'daily') & \n",
    "                                    (rule_performance['TP_Rate'] < 30)].sort_values('Total', ascending=False)\n",
    "    \n",
    "    if not daily_rules.empty:\n",
    "        recommendations.append({\n",
    "            'Category': 'Frequency Adjustment',\n",
    "            'Rules': ', '.join(daily_rules['alert_rules'].head(3)),\n",
    "            'Action': 'Convert inefficient daily rules to weekly frequency',\n",
    "            'Rationale': 'Daily rules generate high volume with low TP rate',\n",
    "            'Impact': 'Reduce alert volume while potentially capturing more meaningful patterns'\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    # Summary visualizations of recommendations\n",
    "    if not recommendations_df.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        recommendations_df['Category'].value_counts().plot(kind='barh', color='skyblue')\n",
    "        plt.title('Recommendations by Category')\n",
    "        plt.xlabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\nGenerated {len(recommendations_df)} recommendations for rule optimization\")\n",
    "    print(recommendations_df)\n",
    "    \n",
    "    return correlation_matrix, high_corr_pairs, recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the rule overlap and recommendations analysis\n",
    "correlation_matrix, high_corr_pairs, recommendations = identify_rule_overlap_and_generate_recommendations(\n",
    "    transaction_data, rule_descriptions, rule_performance, kyc_alerts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Let's save our key findings to Excel for easier sharing and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key results to Excel\n",
    "with pd.ExcelWriter('terrapay_analysis_results.xlsx') as writer:\n",
    "    # Rule performance\n",
    "    rule_performance.to_excel(writer, sheet_name='Rule Performance', index=False)\n",
    "    \n",
    "    # True Positive cases\n",
    "    true_positives.to_excel(writer, sheet_name='True Positives', index=False)\n",
    "    \n",
    "    # KYC breakage analysis\n",
    "    pd.DataFrame({'sender_name': sender_name_groups['sender_name'], \n",
    "                 'kyc_count': sender_name_groups['kyc_id_count']}).to_excel(\n",
    "        writer, sheet_name='Sender KYC Breakage', index=False)\n",
    "    \n",
    "    pd.DataFrame({'receiver_name': receiver_name_groups['receiver_name'], \n",
    "                 'kyc_count': receiver_name_groups['kyc_id_count']}).to_excel(\n",
    "        writer, sheet_name='Receiver KYC Breakage', index=False)\n",
    "    \n",
    "    # Rule correlation matrix\n",
    "    correlation_matrix.to_excel(writer, sheet_name='Rule Correlation Matrix')\n",
    "    \n",
    "    # Recommendations\n",
    "    recommendations.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "\n",
    "print(\"\\nAnalysis complete. Results saved to 'terrapay_analysis_results.xlsx'.\")\n",
    "print(\"Key findings and recommendations have been generated based on the analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Next Steps\n",
    "\n",
    "Based on our analysis, we can draw several conclusions:\n",
    "\n",
    "1. **Rule Efficiency**: We've identified rules with low true positive rates that are generating excessive false positives.\n",
    "\n",
    "2. **KYC Breakage Impact**: The issue of multiple KYC IDs for the same person is significantly affecting rule effectiveness, especially for receiver-focused rules.\n",
    "\n",
    "3. **Rule Overlap**: Several rules are highly correlated and can be consolidated to reduce alert volume without losing effectiveness.\n",
    "\n",
    "4. **Pattern and Frequency Effectiveness**: Different rule patterns and frequencies show varying levels of effectiveness, with some combinations performing better than others.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Implement the recommended rule optimizations, starting with the highest priority items\n",
    "\n",
    "2. Develop a name/phone matching system to mitigate the KYC breakage issue\n",
    "\n",
    "3. Monitor the impact of changes on overall true positive and false positive rates\n",
    "\n",
    "4. Consider more advanced statistical models or machine learning approaches as a future enhancement\n",
    "\n",
    "5. Establish a regular review process to continuously optimize the rules based on ongoing performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}