{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Final Production-Ready Models with Annotated Types\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator, ConfigDict, StringConstraints\n",
    "from typing import Optional, List, Dict, Any, Annotated, Union\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from decimal import Decimal\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ===== ANNOTATED TYPE DEFINITIONS =====\n",
    "\n",
    "# Transaction ID with all constraints\n",
    "TransactionId = Annotated[\n",
    "    str,\n",
    "    StringConstraints(\n",
    "        min_length=16,\n",
    "        max_length=16,\n",
    "        pattern=r'^TP[A-Z]{2}\\d{12}$',\n",
    "        strip_whitespace=True,\n",
    "        to_upper=True\n",
    "    ),\n",
    "    Field(\n",
    "        description=\"Transaction ID in format TP[A-Z]{2}[0-9]{12}\",\n",
    "        examples=[\"TPAB123456789012\", \"TPXY987654321098\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Country and Currency codes\n",
    "CountryCode = Annotated[\n",
    "    str,\n",
    "    StringConstraints(min_length=2, max_length=3, strip_whitespace=True, to_upper=True),\n",
    "    Field(description=\"ISO country code\", examples=[\"US\", \"IN\", \"UK\"])\n",
    "]\n",
    "\n",
    "CurrencyCode = Annotated[\n",
    "    str,\n",
    "    StringConstraints(min_length=3, max_length=3, strip_whitespace=True, to_upper=True),\n",
    "    Field(description=\"ISO currency code\", examples=[\"USD\", \"INR\", \"GBP\"])\n",
    "]\n",
    "\n",
    "# Mandatory string fields\n",
    "MandatoryString = Annotated[\n",
    "    str,\n",
    "    StringConstraints(min_length=1, strip_whitespace=True),\n",
    "    Field(description=\"Non-empty string field\")\n",
    "]\n",
    "\n",
    "# Amount field\n",
    "Amount = Annotated[\n",
    "    float,\n",
    "    Field(gt=0, le=10_000_000, description=\"Transaction amount\", examples=[50000.00, 100000.50])\n",
    "]\n",
    "\n",
    "# Date fields\n",
    "DateFilter = Annotated[\n",
    "    date,\n",
    "    Field(description=\"Date in YYYY-MM-DD format\", examples=[\"2025-06-08\"])\n",
    "]\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "class QueryConfig:\n",
    "    DEFAULT_DAYS_BACK = 30\n",
    "    MAX_DAYS_BACK = 90\n",
    "    MAX_BATCH_SIZE = 20\n",
    "    DEFAULT_TIMEOUT_SECONDS = 30\n",
    "    MAX_AMOUNT = 10_000_000\n",
    "\n",
    "# ===== VALIDATION FUNCTIONS =====\n",
    "\n",
    "def deep_validate_transaction_id(v: str) -> str:\n",
    "    \"\"\"Enhanced transaction ID validation with security checks\"\"\"\n",
    "    if not v:\n",
    "        raise ValueError(\"Transaction ID cannot be empty\")\n",
    "    \n",
    "    # Normalize Unicode (handle lookalikes)\n",
    "    v = unicodedata.normalize('NFKC', v)\n",
    "    \n",
    "    # Remove ALL whitespace and convert to upper\n",
    "    v = ''.join(v.split()).upper()\n",
    "    \n",
    "    # Security checks\n",
    "    dangerous_patterns = [\"'\", '\"', ';', '--', '/*', '*/', '\\x00', '\\n', '\\r', '\\t']\n",
    "    if any(char in v for char in dangerous_patterns):\n",
    "        raise ValueError(f\"Transaction ID contains potentially dangerous characters\")\n",
    "    \n",
    "    # Final format check\n",
    "    if not re.match(r'^TP[A-Z]{2}\\d{12}$', v):\n",
    "        raise ValueError(\n",
    "            f\"Invalid format. Expected: TP + 2 letters + 12 digits, got: '{v}'\"\n",
    "        )\n",
    "    \n",
    "    return v\n",
    "\n",
    "# ===== REQUEST MODELS =====\n",
    "\n",
    "class TransactionIdRequest(BaseModel):\n",
    "    \"\"\"Single transaction lookup request\"\"\"\n",
    "    model_config = ConfigDict(validate_assignment=True)\n",
    "    \n",
    "    transaction_id: TransactionId\n",
    "    start_date: Optional[DateFilter] = Field(\n",
    "        default_factory=lambda: date.today() - timedelta(days=QueryConfig.DEFAULT_DAYS_BACK)\n",
    "    )\n",
    "    end_date: Optional[DateFilter] = Field(\n",
    "        default_factory=date.today\n",
    "    )\n",
    "    search_all_dates: Annotated[\n",
    "        bool,\n",
    "        Field(\n",
    "            default=False,\n",
    "            description=\"Override date range filter (impacts performance)\",\n",
    "            examples=[False, True]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    @field_validator('transaction_id')\n",
    "    @classmethod\n",
    "    def extra_validation(cls, v: str) -> str:\n",
    "        return deep_validate_transaction_id(v)\n",
    "    \n",
    "    @model_validator(mode='after')\n",
    "    def validate_request_logic(self):\n",
    "        \"\"\"Validate request parameters\"\"\"\n",
    "        if not self.search_all_dates:\n",
    "            # Date range validation\n",
    "            if self.start_date > self.end_date:\n",
    "                raise ValueError(\"start_date cannot be after end_date\")\n",
    "            \n",
    "            # Prevent too old queries\n",
    "            max_allowed_start = date.today() - timedelta(days=QueryConfig.MAX_DAYS_BACK)\n",
    "            if self.start_date < max_allowed_start:\n",
    "                raise ValueError(\n",
    "                    f\"start_date older than {QueryConfig.MAX_DAYS_BACK} days. \"\n",
    "                    f\"Set search_all_dates=true to override.\"\n",
    "                )\n",
    "        \n",
    "        return self\n",
    "\n",
    "class BatchTransactionRequest(BaseModel):\n",
    "    \"\"\"Batch transaction lookup request\"\"\"\n",
    "    model_config = ConfigDict(validate_assignment=True)\n",
    "    \n",
    "    transaction_ids: Annotated[\n",
    "        List[TransactionId],\n",
    "        Field(\n",
    "            min_length=1,\n",
    "            max_length=QueryConfig.MAX_BATCH_SIZE,\n",
    "            description=f\"List of transaction IDs (max {QueryConfig.MAX_BATCH_SIZE})\",\n",
    "            examples=[[\"TPAB123456789012\", \"TPCD234567890123\"]]\n",
    "        )\n",
    "    ]\n",
    "    start_date: Optional[DateFilter] = Field(\n",
    "        default_factory=lambda: date.today() - timedelta(days=QueryConfig.DEFAULT_DAYS_BACK)\n",
    "    )\n",
    "    end_date: Optional[DateFilter] = Field(\n",
    "        default_factory=date.today\n",
    "    )\n",
    "    search_all_dates: bool = Field(default=False)\n",
    "    include_warnings: Annotated[\n",
    "        bool,\n",
    "        Field(default=True, description=\"Include data quality warnings\")\n",
    "    ]\n",
    "    timeout_seconds: Annotated[\n",
    "        int,\n",
    "        Field(\n",
    "            default=QueryConfig.DEFAULT_TIMEOUT_SECONDS,\n",
    "            ge=5,\n",
    "            le=60,\n",
    "            description=\"Query timeout in seconds\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    @field_validator('transaction_ids')\n",
    "    @classmethod\n",
    "    def validate_batch_ids(cls, v: List[str]) -> List[str]:\n",
    "        \"\"\"Validate and normalize all IDs\"\"\"\n",
    "        validated = []\n",
    "        for i, txn_id in enumerate(v, 1):\n",
    "            try:\n",
    "                validated.append(deep_validate_transaction_id(txn_id))\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Invalid ID at position {i}: {e}\")\n",
    "        \n",
    "        # Check for duplicates AFTER normalization\n",
    "        unique_ids = set(validated)\n",
    "        if len(unique_ids) != len(validated):\n",
    "            dupes = [id for id in validated if validated.count(id) > 1]\n",
    "            raise ValueError(f\"Duplicate IDs found: {set(dupes)}\")\n",
    "        \n",
    "        return validated\n",
    "\n",
    "# ===== RESPONSE MODELS =====\n",
    "\n",
    "class TransactionResponse(BaseModel):\n",
    "    \"\"\"Transaction details response\"\"\"\n",
    "    model_config = ConfigDict(\n",
    "        populate_by_name=True,\n",
    "        str_strip_whitespace=True,\n",
    "        validate_assignment=True\n",
    "    )\n",
    "    \n",
    "    # Mandatory fields - with proper aliases\n",
    "    transaction_id: Annotated[str, Field(alias=\"hub_transaction_id\")]\n",
    "    response_code: MandatoryString\n",
    "    response_message: MandatoryString\n",
    "    \n",
    "    # Conditionally mandatory\n",
    "    credited_on: Annotated[\n",
    "        Optional[datetime],\n",
    "        Field(\n",
    "            None,\n",
    "            alias=\"modified_on\",\n",
    "            description=\"Credit timestamp (mandatory for success)\",\n",
    "            examples=[\"2025-05-24T10:35:00Z\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Optional fields\n",
    "    created_on: Annotated[\n",
    "        Optional[datetime],\n",
    "        Field(None, alias=\"transaction_date_time_local\")\n",
    "    ]\n",
    "    source_country_code: Optional[CountryCode] = None\n",
    "    destination_country_code: Optional[CountryCode] = None\n",
    "    source_currency_code: Optional[CurrencyCode] = None\n",
    "    destination_currency_code: Optional[CurrencyCode] = None\n",
    "    total_amount_destination: Optional[Amount] = None\n",
    "    sender_name: Annotated[\n",
    "        Optional[str],\n",
    "        Field(None, max_length=500)\n",
    "    ]\n",
    "    receiver_name: Annotated[\n",
    "        Optional[str],\n",
    "        Field(None, max_length=500)\n",
    "    ]\n",
    "    source_partner_id: Optional[str] = None\n",
    "    dest_partner_id: Optional[str] = None\n",
    "    source_partner_name: Optional[str] = None\n",
    "    dest_partner_name: Optional[str] = None\n",
    "    \n",
    "    # Quality tracking\n",
    "    _warnings: List[str] = []\n",
    "    \n",
    "    @field_validator('transaction_id')\n",
    "    @classmethod\n",
    "    def validate_txn_id_response(cls, v: str) -> str:\n",
    "        return deep_validate_transaction_id(v)\n",
    "    \n",
    "    @field_validator('response_code', 'response_message')\n",
    "    @classmethod\n",
    "    def ensure_not_empty(cls, v: Optional[str]) -> str:\n",
    "        if not v or not v.strip():\n",
    "            raise ValueError(\"Cannot be empty or null\")\n",
    "        return v.strip()\n",
    "    \n",
    "    @field_validator('total_amount_destination')\n",
    "    @classmethod\n",
    "    def validate_amount_field(cls, v: Optional[Union[float, Decimal]]) -> Optional[float]:\n",
    "        if v is None:\n",
    "            return None\n",
    "        \n",
    "        # Handle Decimal\n",
    "        v = float(v) if isinstance(v, Decimal) else v\n",
    "        \n",
    "        # Validate range\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"Amount must be positive\")\n",
    "        if v > QueryConfig.MAX_AMOUNT:\n",
    "            raise ValueError(f\"Amount exceeds maximum: {QueryConfig.MAX_AMOUNT}\")\n",
    "        \n",
    "        return round(v, 2)\n",
    "    \n",
    "    @field_validator('created_on', 'credited_on')\n",
    "    @classmethod\n",
    "    def validate_datetime_fields(cls, v: Optional[datetime]) -> Optional[datetime]:\n",
    "        if v is None:\n",
    "            return None\n",
    "        \n",
    "        # Sanity check\n",
    "        if v.year < 2020 or v.year > date.today().year + 1:\n",
    "            raise ValueError(f\"Unrealistic date: {v}\")\n",
    "        \n",
    "        # Ensure timezone aware\n",
    "        if v.tzinfo is None:\n",
    "            v = v.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    @model_validator(mode='after')\n",
    "    def final_business_validation(self):\n",
    "        \"\"\"Business rule validation with warnings\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        # Timestamp consistency\n",
    "        if self.created_on and self.credited_on:\n",
    "            if self.credited_on < self.created_on:\n",
    "                warnings.append(\"credited_on is before created_on\")\n",
    "        \n",
    "        # Success must have credited_on\n",
    "        if \"Remit Success\" in self.response_message and not self.credited_on:\n",
    "            warnings.append(\"Success transaction missing credited_on\")\n",
    "        \n",
    "        # Failed/Delayed should have detailed message\n",
    "        if any(s in self.response_message for s in [\"Remit Failed\", \"Remit Delayed\"]):\n",
    "            if len(self.response_message) < 15:  # \"Remit Failed\" is 12 chars\n",
    "                warnings.append(\"Failed/Delayed transaction has generic message\")\n",
    "        \n",
    "        # In-progress detection\n",
    "        progress_indicators = ['pending', 'processing', 'in progress', 'initiated']\n",
    "        if any(ind in self.response_message.lower() for ind in progress_indicators):\n",
    "            warnings.append(\"Transaction may still be in progress\")\n",
    "        \n",
    "        self._warnings = warnings\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def has_warnings(self) -> bool:\n",
    "        return len(self._warnings) > 0\n",
    "    \n",
    "    @property\n",
    "    def warnings(self) -> List[str]:\n",
    "        return self._warnings.copy()\n",
    "\n",
    "# ===== API RESPONSE WRAPPERS =====\n",
    "\n",
    "class SingleTransactionResponse(BaseModel):\n",
    "    \"\"\"API response for single transaction\"\"\"\n",
    "    success: bool\n",
    "    data: Optional[TransactionResponse] = None\n",
    "    error: Optional[str] = None\n",
    "    processing_time_ms: Annotated[float, Field(ge=0)]\n",
    "    query_metadata: Dict[str, Any] = Field(\n",
    "        default_factory=dict,\n",
    "        examples=[{\"date_range\": {\"start\": \"2025-05-08\", \"end\": \"2025-06-08\"}}]\n",
    "    )\n",
    "\n",
    "class BatchTransactionResponse(BaseModel):\n",
    "    \"\"\"API response for batch transactions\"\"\"\n",
    "    success: bool\n",
    "    summary: Annotated[\n",
    "        Dict[str, int],\n",
    "        Field(examples=[{\"requested\": 5, \"found\": 3, \"not_found\": 2, \"errors\": 0}])\n",
    "    ]\n",
    "    data: Dict[str, TransactionResponse] = Field(default_factory=dict)\n",
    "    errors: Dict[str, str] = Field(default_factory=dict)\n",
    "    warnings: Dict[str, List[str]] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Per-transaction warnings\"\n",
    "    )\n",
    "    processing_time_ms: Annotated[float, Field(ge=0)]\n",
    "    query_metadata: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "# ===== FINAL VALIDATION TESTS =====\n",
    "\n",
    "print(\"Running comprehensive validation tests...\")\n",
    "\n",
    "tests = [\n",
    "    # Valid cases\n",
    "    (\"TPAB123456789012\", True, \"Valid ID\"),\n",
    "    (\"tpab123456789012\", True, \"Lowercase conversion\"),\n",
    "    (\"  TPAB123456789012  \", True, \"Whitespace handling\"),\n",
    "    \n",
    "    # Invalid cases\n",
    "    (\"TPAB12345678901\", False, \"Too short\"),\n",
    "    (\"TPAB1234567890123\", False, \"Too long\"),\n",
    "    (\"ABCD123456789012\", False, \"Wrong prefix\"),\n",
    "    (\"TP12123456789012\", False, \"Numbers in letter position\"),\n",
    "    (\"TPAB12345678901A\", False, \"Letter in number position\"),\n",
    "    (\"TP'; DROP TABLE--\", False, \"SQL injection attempt\"),\n",
    "    (\"ΤPAB123456789012\", False, \"Unicode lookalike\"),\n",
    "    (\"\", False, \"Empty string\"),\n",
    "    (\"TP\\x00AB123456789012\", False, \"Null byte injection\"),\n",
    "]\n",
    "\n",
    "for test_id, should_pass, description in tests:\n",
    "    try:\n",
    "        validated = deep_validate_transaction_id(test_id)\n",
    "        if should_pass:\n",
    "            print(f\"✅ {description}: '{test_id}' → '{validated}'\")\n",
    "        else:\n",
    "            print(f\"❌ {description}: Should have failed but passed!\")\n",
    "    except ValueError as e:\n",
    "        if not should_pass:\n",
    "            print(f\"✅ {description}: Correctly rejected - {e}\")\n",
    "        else:\n",
    "            print(f\"❌ {description}: Should have passed but failed - {e}\")\n",
    "\n",
    "print(\"\\n✅ Final models ready for production use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Database Connection and Query Helpers\n",
    "\n",
    "import time\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from sqlalchemy import text, create_engine, pool\n",
    "from sqlalchemy.engine import Engine\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_CONFIG = {\n",
    "    \"pool_size\": 5,\n",
    "    \"max_overflow\": 10,\n",
    "    \"pool_timeout\": 30,\n",
    "    \"pool_recycle\": 3600,  # Recycle connections after 1 hour\n",
    "    \"echo\": False  # Set to True for SQL debugging\n",
    "}\n",
    "\n",
    "# Create database engine with connection pooling\n",
    "def create_db_engine(database_url: str) -> Engine:\n",
    "    \"\"\"Create SQLAlchemy engine with production-ready configuration\"\"\"\n",
    "    return create_engine(\n",
    "        database_url,\n",
    "        poolclass=pool.QueuePool,\n",
    "        pool_size=DATABASE_CONFIG[\"pool_size\"],\n",
    "        max_overflow=DATABASE_CONFIG[\"max_overflow\"],\n",
    "        pool_timeout=DATABASE_CONFIG[\"pool_timeout\"],\n",
    "        pool_recycle=DATABASE_CONFIG[\"pool_recycle\"],\n",
    "        echo=DATABASE_CONFIG[\"echo\"]\n",
    "    )\n",
    "\n",
    "# Initialize engine (will be set after connection test)\n",
    "engine = None\n",
    "\n",
    "# Database connection context manager\n",
    "@contextmanager\n",
    "def get_db_connection():\n",
    "    \"\"\"Get database connection with automatic cleanup\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = engine.connect()\n",
    "        yield conn\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database connection error: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Query performance tracker\n",
    "class QueryTimer:\n",
    "    \"\"\"Track query execution time\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end_time = time.time()\n",
    "    \n",
    "    @property\n",
    "    def elapsed_ms(self) -> float:\n",
    "        if self.start_time and self.end_time:\n",
    "            return (self.end_time - self.start_time) * 1000\n",
    "        return 0.0\n",
    "\n",
    "# SQL query templates\n",
    "SINGLE_TRANSACTION_QUERY = \"\"\"\n",
    "SELECT \n",
    "    t.hub_transaction_id,\n",
    "    t.response_code,\n",
    "    t.response_message,\n",
    "    t.modified_on,  -- credited_on\n",
    "    t.transaction_date_time_local,  -- created_on\n",
    "    t.source_country_code,\n",
    "    t.destination_country_code,\n",
    "    t.source_currency_code,\n",
    "    t.destination_currency_code,\n",
    "    t.total_amount_destination,\n",
    "    t.sender_name,\n",
    "    t.receiver_name,\n",
    "    t.source_partner_id,\n",
    "    t.dest_partner_id,\n",
    "    sp.partner_name as source_partner_name,\n",
    "    dp.dest_partner_name as dest_partner_name\n",
    "FROM iox_hub_transaction t\n",
    "LEFT JOIN source_partner sp ON t.source_partner_id = sp.source_partner_id\n",
    "LEFT JOIN dest_partner dp ON t.dest_partner_id = dp.dest_partner_id\n",
    "WHERE t.hub_transaction_id = :transaction_id\n",
    "\"\"\"\n",
    "\n",
    "# Add date filter clause\n",
    "DATE_FILTER_CLAUSE = \"\"\"\n",
    "AND t.transaction_date_time_local >= :start_date\n",
    "AND t.transaction_date_time_local <= :end_date\n",
    "\"\"\"\n",
    "\n",
    "# Batch query template\n",
    "BATCH_TRANSACTION_QUERY = \"\"\"\n",
    "SELECT \n",
    "    t.hub_transaction_id,\n",
    "    t.response_code,\n",
    "    t.response_message,\n",
    "    t.modified_on,\n",
    "    t.transaction_date_time_local,\n",
    "    t.source_country_code,\n",
    "    t.destination_country_code,\n",
    "    t.source_currency_code,\n",
    "    t.destination_currency_code,\n",
    "    t.total_amount_destination,\n",
    "    t.sender_name,\n",
    "    t.receiver_name,\n",
    "    t.source_partner_id,\n",
    "    t.dest_partner_id,\n",
    "    sp.partner_name as source_partner_name,\n",
    "    dp.dest_partner_name as dest_partner_name\n",
    "FROM iox_hub_transaction t\n",
    "LEFT JOIN source_partner sp ON t.source_partner_id = sp.source_partner_id\n",
    "LEFT JOIN dest_partner dp ON t.dest_partner_id = dp.dest_partner_id\n",
    "WHERE t.hub_transaction_id = ANY(:transaction_ids)\n",
    "\"\"\"\n",
    "\n",
    "# Test database connection\n",
    "def test_database_connection(database_url: str) -> bool:\n",
    "    \"\"\"Test database connection and verify table structure\"\"\"\n",
    "    global engine\n",
    "    try:\n",
    "        # Create engine\n",
    "        engine = create_db_engine(database_url)\n",
    "        \n",
    "        with get_db_connection() as conn:\n",
    "            # Test basic connectivity\n",
    "            result = conn.execute(text(\"SELECT 1\"))\n",
    "            logger.info(\"✓ Database connection successful\")\n",
    "            \n",
    "            # Verify table exists\n",
    "            table_check = conn.execute(text(\"\"\"\n",
    "                SELECT column_name, data_type \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_name = 'iox_hub_transaction' \n",
    "                AND column_name IN (\n",
    "                    'hub_transaction_id', 'response_code', 'response_message',\n",
    "                    'modified_on', 'transaction_date_time_local'\n",
    "                )\n",
    "                ORDER BY ordinal_position\n",
    "            \"\"\"))\n",
    "            \n",
    "            columns = list(table_check)\n",
    "            logger.info(f\"✓ Found {len(columns)} required columns in iox_hub_transaction\")\n",
    "            \n",
    "            # Get a sample transaction to verify data\n",
    "            sample_check = conn.execute(text(\"\"\"\n",
    "                SELECT hub_transaction_id, response_message \n",
    "                FROM iox_hub_transaction \n",
    "                LIMIT 1\n",
    "            \"\"\"))\n",
    "            \n",
    "            sample = sample_check.fetchone()\n",
    "            if sample:\n",
    "                logger.info(f\"✓ Sample transaction found: {sample[0]}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Database connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Initialize database connection\n",
    "if __name__ == \"__main__\":\n",
    "    DATABASE_URL = \"postgresql://rr_replic_cp:rr_replic_cp!@!$%@127.0.0.1:54326/terra_core_db\"\n",
    "    \n",
    "    if test_database_connection(DATABASE_URL):\n",
    "        print(\"\\n✅ Database setup complete! Ready for queries.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Database setup failed. Please check connection details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Single Transaction Query Implementation\n",
    "\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, Dict, Any\n",
    "import traceback\n",
    "\n",
    "def query_single_transaction(\n",
    "    transaction_id: str,\n",
    "    start_date: Optional[date] = None,\n",
    "    end_date: Optional[date] = None,\n",
    "    search_all_dates: bool = False\n",
    ") -> Tuple[Optional[Dict[str, Any]], Optional[str], float]:\n",
    "    \"\"\"\n",
    "    Query single transaction from database\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (transaction_data, error_message, query_time_ms)\n",
    "    \"\"\"\n",
    "    query_timer = QueryTimer()\n",
    "    \n",
    "    try:\n",
    "        with query_timer:\n",
    "            with get_db_connection() as conn:\n",
    "                # Build query based on date filter preference\n",
    "                if search_all_dates:\n",
    "                    query = SINGLE_TRANSACTION_QUERY\n",
    "                    params = {\"transaction_id\": transaction_id}\n",
    "                else:\n",
    "                    query = SINGLE_TRANSACTION_QUERY + DATE_FILTER_CLAUSE\n",
    "                    params = {\n",
    "                        \"transaction_id\": transaction_id,\n",
    "                        \"start_date\": start_date,\n",
    "                        \"end_date\": end_date\n",
    "                    }\n",
    "                \n",
    "                # Execute query\n",
    "                result = conn.execute(text(query), params)\n",
    "                row = result.fetchone()\n",
    "                \n",
    "                if not row:\n",
    "                    return None, \"Transaction not found\", query_timer.elapsed_ms\n",
    "                \n",
    "                # Convert row to dictionary\n",
    "                transaction_data = dict(row._mapping)\n",
    "                \n",
    "                # Validate critical fields aren't NULL\n",
    "                if not transaction_data.get('response_code'):\n",
    "                    return None, \"Transaction has corrupted data - response_code is NULL\", query_timer.elapsed_ms\n",
    "                \n",
    "                if not transaction_data.get('response_message'):\n",
    "                    return None, \"Transaction has corrupted data - response_message is NULL\", query_timer.elapsed_ms\n",
    "                \n",
    "                return transaction_data, None, query_timer.elapsed_ms\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Database error: {str(e)}\"\n",
    "        logger.error(f\"Query error for {transaction_id}: {error_msg}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None, error_msg, query_timer.elapsed_ms\n",
    "\n",
    "def process_single_transaction_request(request: TransactionIdRequest) -> SingleTransactionResponse:\n",
    "    \"\"\"\n",
    "    Process single transaction request and return formatted response\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Query database\n",
    "        transaction_data, error_msg, db_time = query_single_transaction(\n",
    "            transaction_id=request.transaction_id,\n",
    "            start_date=request.start_date,\n",
    "            end_date=request.end_date,\n",
    "            search_all_dates=request.search_all_dates\n",
    "        )\n",
    "        \n",
    "        # Build metadata\n",
    "        query_metadata = {\n",
    "            \"date_range\": {\n",
    "                \"start\": str(request.start_date),\n",
    "                \"end\": str(request.end_date)\n",
    "            },\n",
    "            \"search_all_dates\": request.search_all_dates,\n",
    "            \"database_response_time_ms\": db_time\n",
    "        }\n",
    "        \n",
    "        if error_msg:\n",
    "            # Error case\n",
    "            return SingleTransactionResponse(\n",
    "                success=False,\n",
    "                data=None,\n",
    "                error=error_msg,\n",
    "                processing_time_ms=(time.time() - start_time) * 1000,\n",
    "                query_metadata=query_metadata\n",
    "            )\n",
    "        \n",
    "        if not transaction_data:\n",
    "            # Not found case\n",
    "            return SingleTransactionResponse(\n",
    "                success=True,\n",
    "                data=None,\n",
    "                error=\"Transaction not found\",\n",
    "                processing_time_ms=(time.time() - start_time) * 1000,\n",
    "                query_metadata=query_metadata\n",
    "            )\n",
    "        \n",
    "        # Success case - create response model\n",
    "        try:\n",
    "            transaction_response = TransactionResponse(**transaction_data)\n",
    "            \n",
    "            # Add warnings to metadata if any\n",
    "            if transaction_response.has_warnings:\n",
    "                query_metadata[\"warnings\"] = transaction_response.warnings\n",
    "            \n",
    "            return SingleTransactionResponse(\n",
    "                success=True,\n",
    "                data=transaction_response,\n",
    "                error=None,\n",
    "                processing_time_ms=(time.time() - start_time) * 1000,\n",
    "                query_metadata=query_metadata\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Data validation error\n",
    "            logger.error(f\"Response validation error: {e}\")\n",
    "            return SingleTransactionResponse(\n",
    "                success=False,\n",
    "                data=None,\n",
    "                error=f\"Data validation error: {str(e)}\",\n",
    "                processing_time_ms=(time.time() - start_time) * 1000,\n",
    "                query_metadata=query_metadata\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Unexpected error\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return SingleTransactionResponse(\n",
    "            success=False,\n",
    "            data=None,\n",
    "            error=f\"Internal error: {str(e)}\",\n",
    "            processing_time_ms=(time.time() - start_time) * 1000,\n",
    "            query_metadata={}\n",
    "        )\n",
    "\n",
    "# Test function\n",
    "def test_single_transaction_query():\n",
    "    \"\"\"Test single transaction query with various scenarios\"\"\"\n",
    "    print(\"Testing single transaction queries...\\n\")\n",
    "    \n",
    "    # Test 1: Valid transaction (you'll need to use a real ID from your database)\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Valid transaction with date range\",\n",
    "            \"request\": TransactionIdRequest(\n",
    "                transaction_id=\"TPAB123456789012\",  # Replace with actual ID\n",
    "                search_all_dates=False\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Valid transaction - all dates\",\n",
    "            \"request\": TransactionIdRequest(\n",
    "                transaction_id=\"TPAB123456789012\",  # Replace with actual ID\n",
    "                search_all_dates=True\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Non-existent transaction\",\n",
    "            \"request\": TransactionIdRequest(\n",
    "                transaction_id=\"TPZZ999999999999\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for test in test_cases:\n",
    "        print(f\"Test: {test['name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        response = process_single_transaction_request(test['request'])\n",
    "        \n",
    "        print(f\"Success: {response.success}\")\n",
    "        print(f\"Processing time: {response.processing_time_ms:.2f}ms\")\n",
    "        \n",
    "        if response.data:\n",
    "            print(f\"Transaction found: {response.data.transaction_id}\")\n",
    "            print(f\"Status: {response.data.response_message}\")\n",
    "            print(f\"Response code: {response.data.response_code}\")\n",
    "            if response.data.credited_on:\n",
    "                print(f\"Credited on: {response.data.credited_on}\")\n",
    "        \n",
    "        if response.error:\n",
    "            print(f\"Error: {response.error}\")\n",
    "        \n",
    "        print(f\"Metadata: {response.query_metadata}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run test if needed\n",
    "# test_single_transaction_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a097e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Batch Transaction Query Implementation\n",
    "\n",
    "from typing import List, Dict, Set\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "\n",
    "def query_batch_transactions(\n",
    "    transaction_ids: List[str],\n",
    "    start_date: Optional[date] = None,\n",
    "    end_date: Optional[date] = None,\n",
    "    search_all_dates: bool = False,\n",
    "    timeout_seconds: int = 30\n",
    ") -> Tuple[Dict[str, Dict], Dict[str, str], float]:\n",
    "    \"\"\"\n",
    "    Query multiple transactions in a single database call\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (transactions_dict, errors_dict, query_time_ms)\n",
    "    \"\"\"\n",
    "    query_timer = QueryTimer()\n",
    "    transactions_dict = {}\n",
    "    errors_dict = {}\n",
    "    \n",
    "    try:\n",
    "        with query_timer:\n",
    "            with get_db_connection() as conn:\n",
    "                # Build query\n",
    "                if search_all_dates:\n",
    "                    query = BATCH_TRANSACTION_QUERY\n",
    "                    params = {\"transaction_ids\": transaction_ids}\n",
    "                else:\n",
    "                    query = BATCH_TRANSACTION_QUERY + DATE_FILTER_CLAUSE\n",
    "                    params = {\n",
    "                        \"transaction_ids\": transaction_ids,\n",
    "                        \"start_date\": start_date,\n",
    "                        \"end_date\": end_date\n",
    "                    }\n",
    "                \n",
    "                # Execute with timeout\n",
    "                result = conn.execute(text(query), params)\n",
    "                rows = result.fetchall()\n",
    "                \n",
    "                # Process found transactions\n",
    "                found_ids = set()\n",
    "                for row in rows:\n",
    "                    try:\n",
    "                        transaction_data = dict(row._mapping)\n",
    "                        txn_id = transaction_data['hub_transaction_id']\n",
    "                        \n",
    "                        # Validate critical fields\n",
    "                        if not transaction_data.get('response_code'):\n",
    "                            errors_dict[txn_id] = \"Transaction has corrupted data - response_code is NULL\"\n",
    "                            continue\n",
    "                        \n",
    "                        if not transaction_data.get('response_message'):\n",
    "                            errors_dict[txn_id] = \"Transaction has corrupted data - response_message is NULL\"\n",
    "                            continue\n",
    "                        \n",
    "                        transactions_dict[txn_id] = transaction_data\n",
    "                        found_ids.add(txn_id)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        txn_id = row[0] if row else \"Unknown\"\n",
    "                        errors_dict[txn_id] = f\"Error processing transaction: {str(e)}\"\n",
    "                \n",
    "                # Identify not found transactions\n",
    "                requested_ids = set(transaction_ids)\n",
    "                not_found_ids = requested_ids - found_ids - set(errors_dict.keys())\n",
    "                \n",
    "                for txn_id in not_found_ids:\n",
    "                    errors_dict[txn_id] = \"Transaction not found\"\n",
    "                \n",
    "                return transactions_dict, errors_dict, query_timer.elapsed_ms\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Database error: {str(e)}\"\n",
    "        logger.error(f\"Batch query error: {error_msg}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        \n",
    "        # Return all as errors\n",
    "        for txn_id in transaction_ids:\n",
    "            if txn_id not in errors_dict:\n",
    "                errors_dict[txn_id] = error_msg\n",
    "        \n",
    "        return transactions_dict, errors_dict, query_timer.elapsed_ms\n",
    "\n",
    "def process_batch_transaction_request(request: BatchTransactionRequest) -> BatchTransactionResponse:\n",
    "    \"\"\"\n",
    "    Process batch transaction request with timeout and validation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize response containers\n",
    "    validated_transactions = {}\n",
    "    all_errors = {}\n",
    "    all_warnings = {}\n",
    "    \n",
    "    try:\n",
    "        # Query database with timeout\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(\n",
    "                query_batch_transactions,\n",
    "                transaction_ids=request.transaction_ids,\n",
    "                start_date=request.start_date,\n",
    "                end_date=request.end_date,\n",
    "                search_all_dates=request.search_all_dates,\n",
    "                timeout_seconds=request.timeout_seconds\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                transactions_dict, errors_dict, db_time = future.result(\n",
    "                    timeout=request.timeout_seconds\n",
    "                )\n",
    "            except TimeoutError:\n",
    "                # Timeout occurred\n",
    "                for txn_id in request.transaction_ids:\n",
    "                    all_errors[txn_id] = \"Query timeout exceeded\"\n",
    "                \n",
    "                return BatchTransactionResponse(\n",
    "                    success=False,\n",
    "                    summary={\n",
    "                        \"requested\": len(request.transaction_ids),\n",
    "                        \"found\": 0,\n",
    "                        \"not_found\": 0,\n",
    "                        \"errors\": len(request.transaction_ids)\n",
    "                    },\n",
    "                    data={},\n",
    "                    errors=all_errors,\n",
    "                    warnings={},\n",
    "                    processing_time_ms=(time.time() - start_time) * 1000,\n",
    "                    query_metadata={\n",
    "                        \"timeout_occurred\": True,\n",
    "                        \"timeout_seconds\": request.timeout_seconds\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        # Process and validate each transaction\n",
    "        for txn_id, txn_data in transactions_dict.items():\n",
    "            try:\n",
    "                # Create response model\n",
    "                transaction_response = TransactionResponse(**txn_data)\n",
    "                validated_transactions[txn_id] = transaction_response\n",
    "                \n",
    "                # Collect warnings if requested\n",
    "                if request.include_warnings and transaction_response.has_warnings:\n",
    "                    all_warnings[txn_id] = transaction_response.warnings\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Validation error\n",
    "                all_errors[txn_id] = f\"Data validation error: {str(e)}\"\n",
    "        \n",
    "        # Add query errors\n",
    "        all_errors.update(errors_dict)\n",
    "        \n",
    "        # Calculate summary\n",
    "        summary = {\n",
    "            \"requested\": len(request.transaction_ids),\n",
    "            \"found\": len(validated_transactions),\n",
    "            \"not_found\": len([e for e in all_errors.values() if e == \"Transaction not found\"]),\n",
    "            \"errors\": len([e for e in all_errors.values() if e != \"Transaction not found\"])\n",
    "        }\n",
    "        \n",
    "        # Build metadata\n",
    "        query_metadata = {\n",
    "            \"date_range\": {\n",
    "                \"start\": str(request.start_date),\n",
    "                \"end\": str(request.end_date)\n",
    "            },\n",
    "            \"search_all_dates\": request.search_all_dates,\n",
    "            \"database_response_time_ms\": db_time,\n",
    "            \"include_warnings\": request.include_warnings\n",
    "        }\n",
    "        \n",
    "        # Determine overall success\n",
    "        success = len(all_errors) == 0 or all(\n",
    "            error == \"Transaction not found\" for error in all_errors.values()\n",
    "        )\n",
    "        \n",
    "        return BatchTransactionResponse(\n",
    "            success=success,\n",
    "            summary=summary,\n",
    "            data=validated_transactions,\n",
    "            errors=all_errors,\n",
    "            warnings=all_warnings,\n",
    "            processing_time_ms=(time.time() - start_time) * 1000,\n",
    "            query_metadata=query_metadata\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Unexpected error\n",
    "        logger.error(f\"Batch processing error: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        \n",
    "        return BatchTransactionResponse(\n",
    "            success=False,\n",
    "            summary={\n",
    "                \"requested\": len(request.transaction_ids),\n",
    "                \"found\": 0,\n",
    "                \"not_found\": 0,\n",
    "                \"errors\": len(request.transaction_ids)\n",
    "            },\n",
    "            data={},\n",
    "            errors={txn_id: f\"Internal error: {str(e)}\" for txn_id in request.transaction_ids},\n",
    "            warnings={},\n",
    "            processing_time_ms=(time.time() - start_time) * 1000,\n",
    "            query_metadata={\"error\": str(e)}\n",
    "        )\n",
    "\n",
    "# Test function for batch queries\n",
    "def test_batch_transaction_query():\n",
    "    \"\"\"Test batch transaction queries with various scenarios\"\"\"\n",
    "    print(\"Testing batch transaction queries...\\n\")\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Small batch - all valid\",\n",
    "            \"request\": BatchTransactionRequest(\n",
    "                transaction_ids=[\n",
    "                    \"TPAB123456789012\",  # Replace with actual IDs\n",
    "                    \"TPCD234567890123\",\n",
    "                    \"TPEF345678901234\"\n",
    "                ]\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Mixed batch - some not found\",\n",
    "            \"request\": BatchTransactionRequest(\n",
    "                transaction_ids=[\n",
    "                    \"TPAB123456789012\",  # Real ID\n",
    "                    \"TPZZ999999999999\",  # Non-existent\n",
    "                    \"TPYY888888888888\"   # Non-existent\n",
    "                ]\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Batch with duplicates\",\n",
    "            \"request\": BatchTransactionRequest(\n",
    "                transaction_ids=[\n",
    "                    \"TPAB123456789012\",\n",
    "                    \"TPAB123456789012\",  # Duplicate\n",
    "                    \"TPCD234567890123\"\n",
    "                ]\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Maximum batch size\",\n",
    "            \"request\": BatchTransactionRequest(\n",
    "                transaction_ids=[f\"TP{chr(65+i//10)}{chr(65+i%10)}{str(i).zfill(12)}\" \n",
    "                               for i in range(20)]\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for test in test_cases:\n",
    "        try:\n",
    "            print(f\"Test: {test['name']}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            response = process_batch_transaction_request(test['request'])\n",
    "            \n",
    "            print(f\"Success: {response.success}\")\n",
    "            print(f\"Summary: {response.summary}\")\n",
    "            print(f\"Processing time: {response.processing_time_ms:.2f}ms\")\n",
    "            \n",
    "            if response.data:\n",
    "                print(f\"\\nFound transactions ({len(response.data)}):\")\n",
    "                for txn_id, txn in list(response.data.items())[:3]:  # Show first 3\n",
    "                    print(f\"  - {txn_id}: {txn.response_message}\")\n",
    "            \n",
    "            if response.errors:\n",
    "                print(f\"\\nErrors ({len(response.errors)}):\")\n",
    "                for txn_id, error in list(response.errors.items())[:3]:  # Show first 3\n",
    "                    print(f\"  - {txn_id}: {error}\")\n",
    "            \n",
    "            if response.warnings:\n",
    "                print(f\"\\nWarnings ({len(response.warnings)}):\")\n",
    "                for txn_id, warnings in list(response.warnings.items())[:3]:  # Show first 3\n",
    "                    print(f\"  - {txn_id}: {warnings}\")\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Test failed with exception: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Run test if needed\n",
    "# test_batch_transaction_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: API Endpoint Simulation and Integration Testing\n",
    "\n",
    "from typing import Union\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Simulate API endpoint functions\n",
    "async def get_single_transaction(\n",
    "    transaction_id: str,\n",
    "    start_date: Optional[str] = None,\n",
    "    end_date: Optional[str] = None,\n",
    "    search_all_dates: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulate GET /api/v1/transactions/{transaction_id} endpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse dates if provided\n",
    "        parsed_start = datetime.strptime(start_date, \"%Y-%m-%d\").date() if start_date else None\n",
    "        parsed_end = datetime.strptime(end_date, \"%Y-%m-%d\").date() if end_date else None\n",
    "        \n",
    "        # Create request model\n",
    "        request = TransactionIdRequest(\n",
    "            transaction_id=transaction_id,\n",
    "            start_date=parsed_start,\n",
    "            end_date=parsed_end,\n",
    "            search_all_dates=search_all_dates\n",
    "        )\n",
    "        \n",
    "        # Process request\n",
    "        response = process_single_transaction_request(request)\n",
    "        \n",
    "        # Convert to JSON-serializable format\n",
    "        return response.model_dump(mode='json')\n",
    "        \n",
    "    except ValueError as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Validation error: {str(e)}\",\n",
    "            \"processing_time_ms\": 0,\n",
    "            \"query_metadata\": {}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Internal server error: {str(e)}\",\n",
    "            \"processing_time_ms\": 0,\n",
    "            \"query_metadata\": {}\n",
    "        }\n",
    "\n",
    "async def post_batch_transactions(\n",
    "    transaction_ids: List[str],\n",
    "    start_date: Optional[str] = None,\n",
    "    end_date: Optional[str] = None,\n",
    "    search_all_dates: bool = False,\n",
    "    include_warnings: bool = True,\n",
    "    timeout_seconds: int = 30\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulate POST /api/v1/transactions/batch endpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse dates if provided\n",
    "        parsed_start = datetime.strptime(start_date, \"%Y-%m-%d\").date() if start_date else None\n",
    "        parsed_end = datetime.strptime(end_date, \"%Y-%m-%d\").date() if end_date else None\n",
    "        \n",
    "        # Create request model\n",
    "        request = BatchTransactionRequest(\n",
    "            transaction_ids=transaction_ids,\n",
    "            start_date=parsed_start,\n",
    "            end_date=parsed_end,\n",
    "            search_all_dates=search_all_dates,\n",
    "            include_warnings=include_warnings,\n",
    "            timeout_seconds=timeout_seconds\n",
    "        )\n",
    "        \n",
    "        # Process request\n",
    "        response = process_batch_transaction_request(request)\n",
    "        \n",
    "        # Convert to JSON-serializable format\n",
    "        return response.model_dump(mode='json')\n",
    "        \n",
    "    except ValueError as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"summary\": {\"requested\": len(transaction_ids), \"found\": 0, \"not_found\": 0, \"errors\": len(transaction_ids)},\n",
    "            \"data\": {},\n",
    "            \"errors\": {tid: f\"Validation error: {str(e)}\" for tid in transaction_ids},\n",
    "            \"warnings\": {},\n",
    "            \"processing_time_ms\": 0,\n",
    "            \"query_metadata\": {}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"summary\": {\"requested\": len(transaction_ids), \"found\": 0, \"not_found\": 0, \"errors\": len(transaction_ids)},\n",
    "            \"data\": {},\n",
    "            \"errors\": {tid: f\"Internal server error: {str(e)}\" for tid in transaction_ids},\n",
    "            \"warnings\": {},\n",
    "            \"processing_time_ms\": 0,\n",
    "            \"query_metadata\": {}\n",
    "        }\n",
    "\n",
    "# Integration test suite\n",
    "class TransactionAPITester:\n",
    "    \"\"\"Comprehensive API testing suite\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_results = []\n",
    "    \n",
    "    async def run_all_tests(self):\n",
    "        \"\"\"Run all integration tests\"\"\"\n",
    "        print(\"🧪 Running Transaction API Integration Tests\\n\")\n",
    "        \n",
    "        # Test 1: Single transaction - success case\n",
    "        await self.test_single_transaction_success()\n",
    "        \n",
    "        # Test 2: Single transaction - not found\n",
    "        await self.test_single_transaction_not_found()\n",
    "        \n",
    "        # Test 3: Single transaction - invalid format\n",
    "        await self.test_single_transaction_invalid()\n",
    "        \n",
    "        # Test 4: Batch - all found\n",
    "        await self.test_batch_all_found()\n",
    "        \n",
    "        # Test 5: Batch - mixed results\n",
    "        await self.test_batch_mixed_results()\n",
    "        \n",
    "        # Test 6: Batch - edge cases\n",
    "        await self.test_batch_edge_cases()\n",
    "        \n",
    "        # Test 7: Performance test\n",
    "        await self.test_performance()\n",
    "        \n",
    "        # Print summary\n",
    "        self.print_summary()\n",
    "    \n",
    "    async def test_single_transaction_success(self):\n",
    "        \"\"\"Test successful single transaction lookup\"\"\"\n",
    "        print(\"Test 1: Single Transaction - Success Case\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Use a real transaction ID from your database\n",
    "        result = await get_single_transaction(\n",
    "            transaction_id=\"TPAB123456789012\",  # Replace with actual ID\n",
    "            search_all_dates=False\n",
    "        )\n",
    "        \n",
    "        self.validate_single_response(result, \"Single Success\")\n",
    "        print(json.dumps(result, indent=2, default=str))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    async def test_single_transaction_not_found(self):\n",
    "        \"\"\"Test single transaction not found\"\"\"\n",
    "        print(\"Test 2: Single Transaction - Not Found\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = await get_single_transaction(\n",
    "            transaction_id=\"TPZZ999999999999\"\n",
    "        )\n",
    "        \n",
    "        assert result[\"success\"] == True  # Request succeeded\n",
    "        assert result[\"error\"] == \"Transaction not found\"\n",
    "        assert result[\"data\"] is None\n",
    "        \n",
    "        print(\"✅ Correctly handled not found case\")\n",
    "        print(f\"Error message: {result['error']}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    async def test_single_transaction_invalid(self):\n",
    "        \"\"\"Test invalid transaction ID format\"\"\"\n",
    "        print(\"Test 3: Single Transaction - Invalid Format\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        test_cases = [\n",
    "            (\"short_id\", \"Too short ID\"),\n",
    "            (\"ABCD123456789012\", \"Wrong prefix\"),\n",
    "            (\"TP12123456789012\", \"Numbers in letter position\"),\n",
    "            (\"TP'; DROP TABLE--\", \"SQL injection attempt\")\n",
    "        ]\n",
    "        \n",
    "        for invalid_id, description in test_cases:\n",
    "            result = await get_single_transaction(transaction_id=invalid_id)\n",
    "            \n",
    "            assert result[\"success\"] == False\n",
    "            assert \"Validation error\" in result[\"error\"]\n",
    "            \n",
    "            print(f\"✅ {description}: {invalid_id}\")\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    async def test_batch_all_found(self):\n",
    "        \"\"\"Test batch with all transactions found\"\"\"\n",
    "        print(\"Test 4: Batch - All Found\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Use real transaction IDs\n",
    "        result = await post_batch_transactions(\n",
    "            transaction_ids=[\n",
    "                \"TPAB123456789012\",  # Replace with actual IDs\n",
    "                \"TPCD234567890123\",\n",
    "                \"TPEF345678901234\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.validate_batch_response(result, \"Batch All Found\")\n",
    "        print(f\"Summary: {result['summary']}\")\n",
    "        print(f\"Processing time: {result['processing_time_ms']:.2f}ms\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    async def test_batch_mixed_results(self):\n",
    "        \"\"\"Test batch with mixed results\"\"\"\n",
    "        print(\"Test 5: Batch - Mixed Results\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = await post_batch_transactions(\n",
    "            transaction_ids=[\n",
    "                \"TPAB123456789012\",  # Real\n",
    "                \"TPZZ999999999999\",  # Not found\n",
    "                \"TPYY888888888888\"   # Not found\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        assert result[\"success\"] == True  # Partial success is still success\n",
    "        assert result[\"summary\"][\"found\"] >= 1\n",
    "        assert result[\"summary\"][\"not_found\"] >= 2\n",
    "        \n",
    "        print(f\"✅ Mixed results handled correctly\")\n",
    "        print(f\"Summary: {result['summary']}\")\n",
    "        print(f\"Errors: {list(result['errors'].keys())}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    async def test_batch_edge_cases(self):\n",
    "        \"\"\"Test batch edge cases\"\"\"\n",
    "        print(\"Test 6: Batch - Edge Cases\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Test max batch size\n",
    "        max_batch = [f\"TP{chr(65+i//10)}{chr(65+i%10)}{str(i).zfill(12)}\" \n",
    "                     for i in range(20)]\n",
    "        \n",
    "        result = await post_batch_transactions(transaction_ids=max_batch)\n",
    "        \n",
    "        assert result[\"summary\"][\"requested\"] == 20\n",
    "        print(f\"✅ Max batch size (20): {result['summary']}\")\n",
    "        \n",
    "        # Test exceeding max batch size\n",
    "        too_large = max_batch + [\"TPXX000000000000\"]\n",
    "        result = await post_batch_transactions(transaction_ids=too_large)\n",
    "        \n",
    "        assert result[\"success\"] == False\n",
    "        assert \"Validation error\" in str(result[\"errors\"])\n",
    "        print(f\"✅ Correctly rejected batch > 20\")\n",
    "        \n",
    "        # Test empty batch\n",
    "        result = await post_batch_transactions(transaction_ids=[])\n",
    "        assert result[\"success\"] == False\n",
    "        print(f\"✅ Correctly rejected empty batch\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    async def test_performance(self):\n",
    "        \"\"\"Test performance metrics\"\"\"\n",
    "        print(\"Test 7: Performance Test\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Single transaction performance\n",
    "        single_times = []\n",
    "        for i in range(5):\n",
    "            result = await get_single_transaction(\n",
    "                transaction_id=f\"TPAB{str(i).zfill(12)}\"\n",
    "            )\n",
    "            single_times.append(result[\"processing_time_ms\"])\n",
    "        \n",
    "        avg_single = sum(single_times) / len(single_times)\n",
    "        print(f\"Average single query time: {avg_single:.2f}ms\")\n",
    "        \n",
    "        # Batch performance\n",
    "        batch_result = await post_batch_transactions(\n",
    "            transaction_ids=[f\"TPAB{str(i).zfill(12)}\" for i in range(10)]\n",
    "        )\n",
    "        \n",
    "        print(f\"Batch query time (10 items): {batch_result['processing_time_ms']:.2f}ms\")\n",
    "        print(f\"Per-transaction time: {batch_result['processing_time_ms']/10:.2f}ms\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    def validate_single_response(self, response: Dict, test_name: str):\n",
    "        \"\"\"Validate single transaction response structure\"\"\"\n",
    "        required_fields = [\"success\", \"processing_time_ms\", \"query_metadata\"]\n",
    "        for field in required_fields:\n",
    "            assert field in response, f\"{test_name}: Missing field {field}\"\n",
    "        \n",
    "        if response[\"success\"] and response.get(\"data\"):\n",
    "            data = response[\"data\"]\n",
    "            assert \"transaction_id\" in data\n",
    "            assert \"response_code\" in data\n",
    "            assert \"response_message\" in data\n",
    "        \n",
    "        self.test_results.append((test_name, \"PASS\"))\n",
    "    \n",
    "    def validate_batch_response(self, response: Dict, test_name: str):\n",
    "        \"\"\"Validate batch response structure\"\"\"\n",
    "        required_fields = [\"success\", \"summary\", \"data\", \"errors\", \"warnings\", \"processing_time_ms\"]\n",
    "        for field in required_fields:\n",
    "            assert field in response, f\"{test_name}: Missing field {field}\"\n",
    "        \n",
    "        summary = response[\"summary\"]\n",
    "        assert summary[\"requested\"] == summary[\"found\"] + summary[\"not_found\"] + summary[\"errors\"]\n",
    "        \n",
    "        self.test_results.append((test_name, \"PASS\"))\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print test summary\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"TEST SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for test_name, status in self.test_results:\n",
    "            print(f\"{test_name}: {status}\")\n",
    "        \n",
    "        total = len(self.test_results)\n",
    "        passed = sum(1 for _, status in self.test_results if status == \"PASS\")\n",
    "        \n",
    "        print(f\"\\nTotal: {total}, Passed: {passed}, Failed: {total - passed}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "# Run integration tests\n",
    "async def run_integration_tests():\n",
    "    \"\"\"Run all integration tests\"\"\"\n",
    "    tester = TransactionAPITester()\n",
    "    await tester.run_all_tests()\n",
    "\n",
    "# For Jupyter notebook - use await directly\n",
    "# await run_integration_tests()\n",
    "\n",
    "# For regular Python script\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(run_integration_tests())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Performance Optimization and Caching Layer\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Simple in-memory cache implementation\n",
    "class TransactionCache:\n",
    "    \"\"\"In-memory cache for transaction data with TTL\"\"\"\n",
    "    \n",
    "    def __init__(self, ttl_seconds: int = 300):  # 5 minutes default\n",
    "        self.cache = {}\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.evictions = 0\n",
    "    \n",
    "    def _make_key(self, transaction_id: str, date_range: Optional[Tuple[date, date]] = None) -> str:\n",
    "        \"\"\"Create cache key from transaction ID and date range\"\"\"\n",
    "        key_parts = [transaction_id]\n",
    "        if date_range:\n",
    "            key_parts.extend([str(date_range[0]), str(date_range[1])])\n",
    "        \n",
    "        key_str = \"|\".join(key_parts)\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, transaction_id: str, date_range: Optional[Tuple[date, date]] = None) -> Optional[Dict]:\n",
    "        \"\"\"Get transaction from cache if not expired\"\"\"\n",
    "        key = self._make_key(transaction_id, date_range)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            cached_data, cached_time = self.cache[key]\n",
    "            \n",
    "            # Check if expired\n",
    "            if datetime.now() - cached_time < timedelta(seconds=self.ttl_seconds):\n",
    "                self.hits += 1\n",
    "                logger.debug(f\"Cache hit for {transaction_id}\")\n",
    "                return cached_data\n",
    "            else:\n",
    "                # Expired - remove from cache\n",
    "                del self.cache[key]\n",
    "                self.evictions += 1\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, transaction_id: str, data: Dict, date_range: Optional[Tuple[date, date]] = None):\n",
    "        \"\"\"Store transaction in cache\"\"\"\n",
    "        key = self._make_key(transaction_id, date_range)\n",
    "        self.cache[key] = (data, datetime.now())\n",
    "        logger.debug(f\"Cached transaction {transaction_id}\")\n",
    "    \n",
    "    def invalidate(self, transaction_id: str):\n",
    "        \"\"\"Remove all cached entries for a transaction ID\"\"\"\n",
    "        keys_to_remove = [k for k in self.cache.keys() if transaction_id in k]\n",
    "        for key in keys_to_remove:\n",
    "            del self.cache[key]\n",
    "            self.evictions += 1\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear entire cache\"\"\"\n",
    "        self.cache.clear()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.evictions = 0\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total_requests = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"size\": len(self.cache),\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"evictions\": self.evictions,\n",
    "            \"hit_rate\": f\"{hit_rate:.2f}%\",\n",
    "            \"ttl_seconds\": self.ttl_seconds\n",
    "        }\n",
    "\n",
    "# Initialize cache\n",
    "transaction_cache = TransactionCache(ttl_seconds=300)\n",
    "\n",
    "# Optimized query functions with caching\n",
    "def query_single_transaction_cached(\n",
    "    transaction_id: str,\n",
    "    start_date: Optional[date] = None,\n",
    "    end_date: Optional[date] = None,\n",
    "    search_all_dates: bool = False,\n",
    "    use_cache: bool = True\n",
    ") -> Tuple[Optional[Dict[str, Any]], Optional[str], float, bool]:\n",
    "    \"\"\"\n",
    "    Query single transaction with caching\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (transaction_data, error_message, query_time_ms, from_cache)\n",
    "    \"\"\"\n",
    "    cache_key_range = None if search_all_dates else (start_date, end_date)\n",
    "    from_cache = False\n",
    "    \n",
    "    # Check cache first\n",
    "    if use_cache:\n",
    "        cached_data = transaction_cache.get(transaction_id, cache_key_range)\n",
    "        if cached_data:\n",
    "            return cached_data, None, 0.0, True\n",
    "    \n",
    "    # Query database\n",
    "    data, error, query_time = query_single_transaction(\n",
    "        transaction_id, start_date, end_date, search_all_dates\n",
    "    )\n",
    "    \n",
    "    # Cache successful results\n",
    "    if use_cache and data and not error:\n",
    "        transaction_cache.set(transaction_id, data, cache_key_range)\n",
    "    \n",
    "    return data, error, query_time, from_cache\n",
    "\n",
    "# Batch query optimization\n",
    "def query_batch_transactions_optimized(\n",
    "    transaction_ids: List[str],\n",
    "    start_date: Optional[date] = None,\n",
    "    end_date: Optional[date] = None,\n",
    "    search_all_dates: bool = False,\n",
    "    timeout_seconds: int = 30,\n",
    "    use_cache: bool = True\n",
    ") -> Tuple[Dict[str, Dict], Dict[str, str], float, Dict[str, bool]]:\n",
    "    \"\"\"\n",
    "    Optimized batch query with cache lookup\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (transactions_dict, errors_dict, query_time_ms, cache_status_dict)\n",
    "    \"\"\"\n",
    "    # First, check cache for each transaction\n",
    "    cached_transactions = {}\n",
    "    uncached_ids = []\n",
    "    cache_status = {}\n",
    "    \n",
    "    if use_cache:\n",
    "        cache_key_range = None if search_all_dates else (start_date, end_date)\n",
    "        \n",
    "        for txn_id in transaction_ids:\n",
    "            cached_data = transaction_cache.get(txn_id, cache_key_range)\n",
    "            if cached_data:\n",
    "                cached_transactions[txn_id] = cached_data\n",
    "                cache_status[txn_id] = True\n",
    "            else:\n",
    "                uncached_ids.append(txn_id)\n",
    "                cache_status[txn_id] = False\n",
    "    else:\n",
    "        uncached_ids = transaction_ids\n",
    "        cache_status = {txn_id: False for txn_id in transaction_ids}\n",
    "    \n",
    "    # If all found in cache, return immediately\n",
    "    if not uncached_ids:\n",
    "        logger.info(f\"All {len(transaction_ids)} transactions found in cache\")\n",
    "        return cached_transactions, {}, 0.0, cache_status\n",
    "    \n",
    "    # Query database for uncached transactions\n",
    "    logger.info(f\"Querying database for {len(uncached_ids)} uncached transactions\")\n",
    "    db_transactions, errors, query_time = query_batch_transactions(\n",
    "        uncached_ids, start_date, end_date, search_all_dates, timeout_seconds\n",
    "    )\n",
    "    \n",
    "    # Cache the newly fetched transactions\n",
    "    if use_cache:\n",
    "        for txn_id, txn_data in db_transactions.items():\n",
    "            transaction_cache.set(txn_id, txn_data, cache_key_range)\n",
    "    \n",
    "    # Combine cached and fresh data\n",
    "    all_transactions = {**cached_transactions, **db_transactions}\n",
    "    \n",
    "    return all_transactions, errors, query_time, cache_status\n",
    "\n",
    "# Query optimization strategies\n",
    "class QueryOptimizer:\n",
    "    \"\"\"Optimize queries based on patterns and statistics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.query_stats = {\n",
    "            \"single_queries\": 0,\n",
    "            \"batch_queries\": 0,\n",
    "            \"avg_batch_size\": 0,\n",
    "            \"common_date_ranges\": {},\n",
    "            \"frequent_transactions\": {}\n",
    "        }\n",
    "    \n",
    "    def analyze_query_pattern(self, transaction_ids: List[str], date_range: Tuple[date, date]):\n",
    "        \"\"\"Analyze query patterns for optimization\"\"\"\n",
    "        # Track frequency\n",
    "        for txn_id in transaction_ids:\n",
    "            self.query_stats[\"frequent_transactions\"][txn_id] = \\\n",
    "                self.query_stats[\"frequent_transactions\"].get(txn_id, 0) + 1\n",
    "        \n",
    "        # Track date ranges\n",
    "        range_key = f\"{date_range[0]}_{date_range[1]}\"\n",
    "        self.query_stats[\"common_date_ranges\"][range_key] = \\\n",
    "            self.query_stats[\"common_date_ranges\"].get(range_key, 0) + 1\n",
    "    \n",
    "    def get_optimization_hints(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get optimization recommendations\"\"\"\n",
    "        hints = {\n",
    "            \"cache_recommendations\": [],\n",
    "            \"index_recommendations\": [],\n",
    "            \"query_recommendations\": []\n",
    "        }\n",
    "        \n",
    "        # Identify hot transactions\n",
    "        hot_transactions = sorted(\n",
    "            self.query_stats[\"frequent_transactions\"].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:10]\n",
    "        \n",
    "        if hot_transactions:\n",
    "            hints[\"cache_recommendations\"].append(\n",
    "                f\"Consider longer TTL for frequently accessed transactions: {[t[0] for t in hot_transactions[:5]]}\"\n",
    "            )\n",
    "        \n",
    "        # Date range analysis\n",
    "        common_ranges = sorted(\n",
    "            self.query_stats[\"common_date_ranges\"].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        if common_ranges:\n",
    "            hints[\"index_recommendations\"].append(\n",
    "                \"Consider partial index on transaction_date_time_local for common date ranges\"\n",
    "            )\n",
    "        \n",
    "        return hints\n",
    "\n",
    "# Initialize optimizer\n",
    "query_optimizer = QueryOptimizer()\n",
    "\n",
    "# Performance monitoring decorator\n",
    "def monitor_performance(func):\n",
    "    \"\"\"Decorator to monitor query performance\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Log slow queries\n",
    "        if elapsed > 100:  # More than 100ms\n",
    "            logger.warning(f\"Slow query detected: {func.__name__} took {elapsed:.2f}ms\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Test performance improvements\n",
    "def test_cache_performance():\n",
    "    \"\"\"Test cache performance improvements\"\"\"\n",
    "    print(\"🚀 Testing Performance Optimizations\\n\")\n",
    "    \n",
    "    # Test transaction IDs\n",
    "    test_ids = [\"TPAB123456789012\", \"TPCD234567890123\", \"TPEF345678901234\"]\n",
    "    \n",
    "    print(\"1. Testing single query caching:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # First query - cache miss\n",
    "    _, _, time1, from_cache1 = query_single_transaction_cached(test_ids[0])\n",
    "    print(f\"First query: {time1:.2f}ms (from_cache: {from_cache1})\")\n",
    "    \n",
    "    # Second query - cache hit\n",
    "    _, _, time2, from_cache2 = query_single_transaction_cached(test_ids[0])\n",
    "    print(f\"Second query: {time2:.2f}ms (from_cache: {from_cache2})\")\n",
    "    \n",
    "    print(f\"Speed improvement: {time1/max(time2, 0.1):.1f}x faster\")\n",
    "    \n",
    "    print(\"\\n2. Testing batch query optimization:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Clear cache for fair test\n",
    "    transaction_cache.clear()\n",
    "    \n",
    "    # First batch query\n",
    "    _, _, time1, cache_status1 = query_batch_transactions_optimized(test_ids)\n",
    "    cached_count1 = sum(1 for v in cache_status1.values() if v)\n",
    "    print(f\"First batch: {time1:.2f}ms ({cached_count1}/{len(test_ids)} from cache)\")\n",
    "    \n",
    "    # Second batch query with overlap\n",
    "    test_ids_overlap = test_ids[:2] + [\"TPGH456789012345\"]\n",
    "    _, _, time2, cache_status2 = query_batch_transactions_optimized(test_ids_overlap)\n",
    "    cached_count2 = sum(1 for v in cache_status2.values() if v)\n",
    "    print(f\"Second batch: {time2:.2f}ms ({cached_count2}/{len(test_ids_overlap)} from cache)\")\n",
    "    \n",
    "    print(\"\\n3. Cache Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    stats = transaction_cache.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n4. Query Pattern Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Simulate some query patterns\n",
    "    for i in range(5):\n",
    "        query_optimizer.analyze_query_pattern(\n",
    "            [test_ids[0]], \n",
    "            (date.today() - timedelta(days=30), date.today())\n",
    "        )\n",
    "    \n",
    "    hints = query_optimizer.get_optimization_hints()\n",
    "    print(\"Optimization hints:\")\n",
    "    for category, recommendations in hints.items():\n",
    "        if recommendations:\n",
    "            print(f\"\\n{category}:\")\n",
    "            for rec in recommendations:\n",
    "                print(f\"  - {rec}\")\n",
    "\n",
    "# Run performance tests\n",
    "test_cache_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Production-Ready FastAPI Implementation\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query, Path, Depends, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from contextlib import asynccontextmanager\n",
    "import uvicorn\n",
    "from typing import Optional\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Create lifecycle manager for FastAPI\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Manage application lifecycle - startup and shutdown\"\"\"\n",
    "    # Startup\n",
    "    print(\"🚀 Starting Transaction Query API...\")\n",
    "    \n",
    "    # Initialize database connection\n",
    "    global engine\n",
    "    DATABASE_URL = \"postgresql://rr_replic_cp:rr_replic_cp!@!$%@127.0.0.1:54326/terra_core_db\"\n",
    "    \n",
    "    if not test_database_connection(DATABASE_URL):\n",
    "        raise RuntimeError(\"Failed to connect to database\")\n",
    "    \n",
    "    print(\"✅ Database connected\")\n",
    "    print(f\"✅ Cache initialized with {transaction_cache.ttl_seconds}s TTL\")\n",
    "    print(\"✅ API ready to serve requests\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Shutdown\n",
    "    print(\"\\n🛑 Shutting down Transaction Query API...\")\n",
    "    if engine:\n",
    "        engine.dispose()\n",
    "    print(\"✅ Cleanup complete\")\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Transaction Query API\",\n",
    "    description=\"Production-ready API for querying transaction status\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan,\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"GET\", \"POST\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Request ID middleware for tracking\n",
    "@app.middleware(\"http\")\n",
    "async def add_request_id(request: Request, call_next):\n",
    "    \"\"\"Add request ID for tracking\"\"\"\n",
    "    import uuid\n",
    "    request_id = str(uuid.uuid4())\n",
    "    request.state.request_id = request_id\n",
    "    \n",
    "    response = await call_next(request)\n",
    "    response.headers[\"X-Request-ID\"] = request_id\n",
    "    return response\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/health\", tags=[\"Health\"])\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check database connectivity\n",
    "        with get_db_connection() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        \n",
    "        cache_stats = transaction_cache.get_stats()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"database\": \"connected\",\n",
    "            \"cache\": cache_stats\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return JSONResponse(\n",
    "            status_code=503,\n",
    "            content={\n",
    "                \"status\": \"unhealthy\",\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Single transaction endpoint\n",
    "@app.get(\n",
    "    \"/api/v1/transactions/{transaction_id}\",\n",
    "    response_model=SingleTransactionResponse,\n",
    "    tags=[\"Transactions\"],\n",
    "    summary=\"Get single transaction\",\n",
    "    description=\"Retrieve details of a single transaction by ID\"\n",
    ")\n",
    "async def get_transaction(\n",
    "    transaction_id: str = Path(\n",
    "        ...,\n",
    "        description=\"Transaction ID in format TP[A-Z]{2}[0-9]{12}\",\n",
    "        example=\"TPAB123456789012\"\n",
    "    ),\n",
    "    start_date: Optional[str] = Query(\n",
    "        None,\n",
    "        description=\"Start date for search range (YYYY-MM-DD)\",\n",
    "        example=\"2025-05-01\"\n",
    "    ),\n",
    "    end_date: Optional[str] = Query(\n",
    "        None,\n",
    "        description=\"End date for search range (YYYY-MM-DD)\",\n",
    "        example=\"2025-06-08\"\n",
    "    ),\n",
    "    search_all_dates: bool = Query(\n",
    "        False,\n",
    "        description=\"Search without date restrictions (impacts performance)\"\n",
    "    ),\n",
    "    use_cache: bool = Query(\n",
    "        True,\n",
    "        description=\"Use cache for faster response\"\n",
    "    )\n",
    "):\n",
    "    \"\"\"Get single transaction details\"\"\"\n",
    "    try:\n",
    "        # Parse dates\n",
    "        parsed_start = datetime.strptime(start_date, \"%Y-%m-%d\").date() if start_date else None\n",
    "        parsed_end = datetime.strptime(end_date, \"%Y-%m-%d\").date() if end_date else None\n",
    "        \n",
    "        # Create request model\n",
    "        request = TransactionIdRequest(\n",
    "            transaction_id=transaction_id,\n",
    "            start_date=parsed_start,\n",
    "            end_date=parsed_end,\n",
    "            search_all_dates=search_all_dates\n",
    "        )\n",
    "        \n",
    "        # Query with caching\n",
    "        data, error, query_time, from_cache = query_single_transaction_cached(\n",
    "            transaction_id=request.transaction_id,\n",
    "            start_date=request.start_date,\n",
    "            end_date=request.end_date,\n",
    "            search_all_dates=request.search_all_dates,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        \n",
    "        # Build response\n",
    "        query_metadata = {\n",
    "            \"date_range\": {\n",
    "                \"start\": str(request.start_date),\n",
    "                \"end\": str(request.end_date)\n",
    "            },\n",
    "            \"search_all_dates\": request.search_all_dates,\n",
    "            \"from_cache\": from_cache,\n",
    "            \"database_response_time_ms\": query_time if not from_cache else None\n",
    "        }\n",
    "        \n",
    "        if error:\n",
    "            return SingleTransactionResponse(\n",
    "                success=False,\n",
    "                data=None,\n",
    "                error=error,\n",
    "                processing_time_ms=query_time,\n",
    "                query_metadata=query_metadata\n",
    "            )\n",
    "        \n",
    "        if not data:\n",
    "            return SingleTransactionResponse(\n",
    "                success=True,\n",
    "                data=None,\n",
    "                error=\"Transaction not found\",\n",
    "                processing_time_ms=query_time,\n",
    "                query_metadata=query_metadata\n",
    "            )\n",
    "        \n",
    "        # Create response model\n",
    "        transaction_response = TransactionResponse(**data)\n",
    "        \n",
    "        return SingleTransactionResponse(\n",
    "            success=True,\n",
    "            data=transaction_response,\n",
    "            error=None,\n",
    "            processing_time_ms=query_time,\n",
    "            query_metadata=query_metadata\n",
    "        )\n",
    "        \n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Validation error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing request: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "# Batch transaction endpoint\n",
    "@app.post(\n",
    "    \"/api/v1/transactions/batch\",\n",
    "    response_model=BatchTransactionResponse,\n",
    "    tags=[\"Transactions\"],\n",
    "    summary=\"Get multiple transactions\",\n",
    "    description=\"Retrieve details of multiple transactions in a single request\"\n",
    ")\n",
    "async def get_batch_transactions(\n",
    "    request: BatchTransactionRequest\n",
    "):\n",
    "    \"\"\"Get multiple transaction details in batch\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Query with optimization\n",
    "        transactions_dict, errors_dict, query_time, cache_status = query_batch_transactions_optimized(\n",
    "            transaction_ids=request.transaction_ids,\n",
    "            start_date=request.start_date,\n",
    "            end_date=request.end_date,\n",
    "            search_all_dates=request.search_all_dates,\n",
    "            timeout_seconds=request.timeout_seconds,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        # Process and validate transactions\n",
    "        validated_transactions = {}\n",
    "        validation_errors = {}\n",
    "        warnings = {}\n",
    "        \n",
    "        for txn_id, txn_data in transactions_dict.items():\n",
    "            try:\n",
    "                transaction_response = TransactionResponse(**txn_data)\n",
    "                validated_transactions[txn_id] = transaction_response\n",
    "                \n",
    "                if request.include_warnings and transaction_response.has_warnings:\n",
    "                    warnings[txn_id] = transaction_response.warnings\n",
    "                    \n",
    "            except Exception as e:\n",
    "                validation_errors[txn_id] = f\"Data validation error: {str(e)}\"\n",
    "        \n",
    "        # Combine all errors\n",
    "        all_errors = {**errors_dict, **validation_errors}\n",
    "        \n",
    "        # Calculate summary\n",
    "        cache_hits = sum(1 for hit in cache_status.values() if hit)\n",
    "        summary = {\n",
    "            \"requested\": len(request.transaction_ids),\n",
    "            \"found\": len(validated_transactions),\n",
    "            \"not_found\": len([e for e in all_errors.values() if e == \"Transaction not found\"]),\n",
    "            \"errors\": len([e for e in all_errors.values() if e != \"Transaction not found\"])\n",
    "        }\n",
    "        \n",
    "        # Build metadata\n",
    "        query_metadata = {\n",
    "            \"date_range\": {\n",
    "                \"start\": str(request.start_date),\n",
    "                \"end\": str(request.end_date)\n",
    "            },\n",
    "            \"search_all_dates\": request.search_all_dates,\n",
    "            \"cache_hits\": cache_hits,\n",
    "            \"cache_misses\": len(request.transaction_ids) - cache_hits,\n",
    "            \"database_response_time_ms\": query_time\n",
    "        }\n",
    "        \n",
    "        return BatchTransactionResponse(\n",
    "            success=len(validation_errors) == 0,\n",
    "            summary=summary,\n",
    "            data=validated_transactions,\n",
    "            errors=all_errors,\n",
    "            warnings=warnings,\n",
    "            processing_time_ms=(time.time() - start_time) * 1000,\n",
    "            query_metadata=query_metadata\n",
    "        )\n",
    "        \n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Validation error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch processing error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "# Cache management endpoints\n",
    "@app.post(\"/api/v1/cache/clear\", tags=[\"Cache Management\"])\n",
    "async def clear_cache():\n",
    "    \"\"\"Clear transaction cache\"\"\"\n",
    "    transaction_cache.clear()\n",
    "    return {\"message\": \"Cache cleared successfully\"}\n",
    "\n",
    "@app.get(\"/api/v1/cache/stats\", tags=[\"Cache Management\"])\n",
    "async def get_cache_stats():\n",
    "    \"\"\"Get cache statistics\"\"\"\n",
    "    return transaction_cache.get_stats()\n",
    "\n",
    "# API documentation models for OpenAPI\n",
    "from pydantic import BaseModel as PydanticBaseModel\n",
    "\n",
    "class ErrorResponse(PydanticBaseModel):\n",
    "    \"\"\"Error response model\"\"\"\n",
    "    detail: str\n",
    "    \n",
    "# Configure API responses\n",
    "responses = {\n",
    "    400: {\"description\": \"Bad Request\", \"model\": ErrorResponse},\n",
    "    404: {\"description\": \"Not Found\", \"model\": ErrorResponse},\n",
    "    500: {\"description\": \"Internal Server Error\", \"model\": ErrorResponse},\n",
    "}\n",
    "\n",
    "# Update endpoints with response documentation\n",
    "for route in app.routes:\n",
    "    if hasattr(route, \"responses\"):\n",
    "        route.responses.update(responses)\n",
    "\n",
    "# Main entry point for testing\n",
    "def run_api(host: str = \"0.0.0.0\", port: int = 8000):\n",
    "    \"\"\"Run the API server\"\"\"\n",
    "    uvicorn.run(\n",
    "        \"app:app\",  # Adjust based on your module name\n",
    "        host=host,\n",
    "        port=port,\n",
    "        reload=True,\n",
    "        log_level=\"info\"\n",
    "    )\n",
    "\n",
    "# API client for testing\n",
    "class TransactionAPIClient:\n",
    "    \"\"\"Client for testing the API\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    async def test_endpoints(self):\n",
    "        \"\"\"Test all API endpoints\"\"\"\n",
    "        import aiohttp\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Test health\n",
    "            async with session.get(f\"{self.base_url}/health\") as resp:\n",
    "                print(f\"Health check: {resp.status}\")\n",
    "                print(await resp.json())\n",
    "            \n",
    "            # Test single transaction\n",
    "            async with session.get(\n",
    "                f\"{self.base_url}/api/v1/transactions/TPAB123456789012\"\n",
    "            ) as resp:\n",
    "                print(f\"\\nSingle transaction: {resp.status}\")\n",
    "                result = await resp.json()\n",
    "                print(f\"Success: {result.get('success')}\")\n",
    "                if result.get('data'):\n",
    "                    print(f\"Transaction: {result['data']['transaction_id']}\")\n",
    "            \n",
    "            # Test batch\n",
    "            async with session.post(\n",
    "                f\"{self.base_url}/api/v1/transactions/batch\",\n",
    "                json={\n",
    "                    \"transaction_ids\": [\"TPAB123456789012\", \"TPCD234567890123\"]\n",
    "                }\n",
    "            ) as resp:\n",
    "                print(f\"\\nBatch transaction: {resp.status}\")\n",
    "                result = await resp.json()\n",
    "                print(f\"Summary: {result.get('summary')}\")\n",
    "\n",
    "# Print API startup message\n",
    "print(\"\"\"\n",
    "🚀 FastAPI Transaction Query API Ready!\n",
    "\n",
    "To run the API:\n",
    "1. In terminal: uvicorn app:app --reload\n",
    "2. In Jupyter: await run_api() in async cell\n",
    "3. Access docs: http://localhost:8000/docs\n",
    "\n",
    "Example usage:\n",
    "- GET /api/v1/transactions/TPAB123456789012\n",
    "- POST /api/v1/transactions/batch\n",
    "  Body: {\"transaction_ids\": [\"TPAB123456789012\", \"TPCD234567890123\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd76d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Docker Configuration and Deployment Setup\n",
    "\n",
    "# Create Dockerfile content\n",
    "dockerfile_content = \"\"\"\n",
    "# Use Python 3.11 slim image\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    postgresql-client \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first for better caching\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run the application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "# Create requirements.txt content\n",
    "requirements_content = \"\"\"\n",
    "fastapi==0.109.0\n",
    "uvicorn[standard]==0.27.0\n",
    "pydantic==2.5.3\n",
    "sqlalchemy==2.0.25\n",
    "psycopg2-binary==2.9.9\n",
    "python-dateutil==2.8.2\n",
    "python-multipart==0.0.6\n",
    "asyncpg==0.29.0\n",
    "redis==5.0.1\n",
    "prometheus-client==0.19.0\n",
    "python-json-logger==2.0.7\n",
    "\"\"\"\n",
    "\n",
    "# Create docker-compose.yml content\n",
    "docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    container_name: transaction-api\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://rr_replic_cp:rr_replic_cp!@!$%@postgres:5432/terra_core_db\n",
    "      - REDIS_URL=redis://redis:6379\n",
    "      - LOG_LEVEL=INFO\n",
    "      - WORKERS=4\n",
    "    depends_on:\n",
    "      - postgres\n",
    "      - redis\n",
    "    networks:\n",
    "      - app-network\n",
    "    restart: unless-stopped\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:15-alpine\n",
    "    container_name: transaction-db\n",
    "    environment:\n",
    "      - POSTGRES_USER=rr_replic_cp\n",
    "      - POSTGRES_PASSWORD=rr_replic_cp!@!$%\n",
    "      - POSTGRES_DB=terra_core_db\n",
    "    ports:\n",
    "      - \"54326:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    networks:\n",
    "      - app-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    container_name: transaction-cache\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    networks:\n",
    "      - app-network\n",
    "    restart: unless-stopped\n",
    "    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus\n",
    "    container_name: transaction-metrics\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus_data:/prometheus\n",
    "    networks:\n",
    "      - app-network\n",
    "    restart: unless-stopped\n",
    "\n",
    "networks:\n",
    "  app-network:\n",
    "    driver: bridge\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  prometheus_data:\n",
    "\"\"\"\n",
    "\n",
    "# Create main.py - Production ready application\n",
    "main_py_content = '''\n",
    "import os\n",
    "import logging\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.middleware.trustedhost import TrustedHostMiddleware\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "from fastapi.responses import PlainTextResponse\n",
    "import time\n",
    "import json\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "# Configure structured logging\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "logHandler.setFormatter(formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logHandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Metrics\n",
    "REQUEST_COUNT = Counter(\n",
    "    'http_requests_total', \n",
    "    'Total HTTP requests', \n",
    "    ['method', 'endpoint', 'status']\n",
    ")\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'http_request_duration_seconds', \n",
    "    'HTTP request latency'\n",
    ")\n",
    "\n",
    "# Import your models and functions here\n",
    "# from models import *\n",
    "# from database import *\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Application lifecycle management\"\"\"\n",
    "    logger.info(\"Starting Transaction Query API\", extra={\n",
    "        \"event\": \"startup\",\n",
    "        \"workers\": os.getenv(\"WORKERS\", 1)\n",
    "    })\n",
    "    \n",
    "    # Initialize database\n",
    "    # Initialize cache\n",
    "    # Initialize other services\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup\n",
    "    logger.info(\"Shutting down Transaction Query API\", extra={\n",
    "        \"event\": \"shutdown\"\n",
    "    })\n",
    "\n",
    "# Create app\n",
    "app = FastAPI(\n",
    "    title=\"Transaction Query API\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# Security middleware\n",
    "app.add_middleware(\n",
    "    TrustedHostMiddleware,\n",
    "    allowed_hosts=[\"*\"]  # Configure for your domain\n",
    ")\n",
    "\n",
    "# CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=os.getenv(\"CORS_ORIGINS\", \"*\").split(\",\"),\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"GET\", \"POST\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Metrics middleware\n",
    "@app.middleware(\"http\")\n",
    "async def metrics_middleware(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = await call_next(request)\n",
    "    \n",
    "    REQUEST_COUNT.labels(\n",
    "        method=request.method,\n",
    "        endpoint=request.url.path,\n",
    "        status=response.status_code\n",
    "    ).inc()\n",
    "    \n",
    "    REQUEST_LATENCY.observe(time.time() - start_time)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Logging middleware\n",
    "@app.middleware(\"http\")\n",
    "async def logging_middleware(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = await call_next(request)\n",
    "    \n",
    "    logger.info(\"Request processed\", extra={\n",
    "        \"method\": request.method,\n",
    "        \"path\": request.url.path,\n",
    "        \"status_code\": response.status_code,\n",
    "        \"duration_ms\": (time.time() - start_time) * 1000,\n",
    "        \"client_ip\": request.client.host\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Metrics endpoint\n",
    "@app.get(\"/metrics\", include_in_schema=False)\n",
    "async def metrics():\n",
    "    return PlainTextResponse(generate_latest())\n",
    "\n",
    "# Add your endpoints here\n",
    "# app.include_router(transaction_router, prefix=\"/api/v1\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\n",
    "        \"main:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=int(os.getenv(\"PORT\", 8000)),\n",
    "        workers=int(os.getenv(\"WORKERS\", 4)),\n",
    "        log_config=None  # Use our custom logging\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Create deployment files\n",
    "def create_deployment_files():\n",
    "    \"\"\"Create all deployment configuration files\"\"\"\n",
    "    \n",
    "    files = {\n",
    "        \"Dockerfile\": dockerfile_content,\n",
    "        \"requirements.txt\": requirements_content,\n",
    "        \"docker-compose.yml\": docker_compose_content,\n",
    "        \"main.py\": main_py_content,\n",
    "        \".dockerignore\": \"\"\"\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "env/\n",
    "venv/\n",
    ".venv/\n",
    "pip-log.txt\n",
    "pip-delete-this-directory.txt\n",
    ".tox/\n",
    ".coverage\n",
    ".coverage.*\n",
    ".cache\n",
    "nosetests.xml\n",
    "coverage.xml\n",
    "*.cover\n",
    "*.log\n",
    ".git\n",
    ".gitignore\n",
    ".mypy_cache\n",
    ".pytest_cache\n",
    ".hypothesis\n",
    ".ipynb_checkpoints\n",
    "\"\"\"\n",
    "    }\n",
    "    \n",
    "    print(\"📁 Creating deployment files:\\n\")\n",
    "    for filename, content in files.items():\n",
    "        print(f\"✅ {filename}\")\n",
    "        # Uncomment to actually create files\n",
    "        # with open(filename, 'w') as f:\n",
    "        #     f.write(content.strip())\n",
    "    \n",
    "    print(\"\\n📋 Deployment Instructions:\")\n",
    "    print(\"1. Build: docker-compose build\")\n",
    "    print(\"2. Run: docker-compose up -d\")\n",
    "    print(\"3. Check logs: docker-compose logs -f api\")\n",
    "    print(\"4. Scale: docker-compose up -d --scale api=3\")\n",
    "\n",
    "# Kubernetes configuration\n",
    "kubernetes_yaml = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: transaction-api\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: transaction-api\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: transaction-api\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: api\n",
    "        image: transaction-api:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: DATABASE_URL\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: db-secret\n",
    "              key: url\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: transaction-api-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: transaction-api\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: transaction-api-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: transaction-api\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "🚀 Deployment Configuration Ready!\n",
    "\n",
    "Key Features:\n",
    "1. ✅ Dockerized application\n",
    "2. ✅ Docker Compose for local development\n",
    "3. ✅ Production-ready logging\n",
    "4. ✅ Prometheus metrics\n",
    "5. ✅ Health checks\n",
    "6. ✅ Kubernetes ready\n",
    "7. ✅ Auto-scaling configuration\n",
    "8. ✅ Resource limits\n",
    "9. ✅ Security best practices\n",
    "\n",
    "Next Steps:\n",
    "1. Review and adjust configuration\n",
    "2. Set up CI/CD pipeline\n",
    "3. Configure monitoring dashboards\n",
    "4. Set up alerts\n",
    "5. Load testing\n",
    "6. Security scanning\n",
    "\"\"\")\n",
    "\n",
    "# Show deployment checklist\n",
    "deployment_checklist = \"\"\"\n",
    "📋 Production Deployment Checklist:\n",
    "\n",
    "□ Environment Variables\n",
    "  - DATABASE_URL (with connection pooling)\n",
    "  - REDIS_URL (for distributed caching)\n",
    "  - API_KEY (for authentication)\n",
    "  - LOG_LEVEL\n",
    "  \n",
    "□ Database\n",
    "  - Connection pooling configured\n",
    "  - Read replicas for scaling\n",
    "  - Backup strategy\n",
    "  - Migration scripts\n",
    "  \n",
    "□ Security\n",
    "  - API authentication\n",
    "  - Rate limiting\n",
    "  - Input validation\n",
    "  - SQL injection prevention\n",
    "  - HTTPS/TLS\n",
    "  \n",
    "□ Monitoring\n",
    "  - Prometheus metrics\n",
    "  - Grafana dashboards\n",
    "  - Alert rules\n",
    "  - Log aggregation (ELK/EFK)\n",
    "  \n",
    "□ Performance\n",
    "  - Load testing completed\n",
    "  - Caching strategy\n",
    "  - Database indexes\n",
    "  - Query optimization\n",
    "  \n",
    "□ Operations\n",
    "  - CI/CD pipeline\n",
    "  - Rollback strategy\n",
    "  - Documentation\n",
    "  - Runbooks\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_checklist)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
